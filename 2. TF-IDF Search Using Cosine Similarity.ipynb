{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5dcf6d",
   "metadata": {},
   "source": [
    "# M1 Text Processing Using spaCy Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0b690",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Preprocess the dataset using spaCy library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65482bb8",
   "metadata": {},
   "source": [
    "## Load all relevant Python libraries and a spaCy language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e32ac537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1fd9b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "16d04b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = sp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b0e9e4",
   "metadata": {},
   "source": [
    "##  Open the provided JSON file. \n",
    "\n",
    "It contains a list of dictionaries with summaries from Wikipedia articles, where each dictionary has three key-value pairs. The keys title, text and url correspond to:\n",
    "\n",
    "- Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- Link to the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "0e9e4116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'text', 'url'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/data.json', 'r') as outfile:\n",
    "    summaries = json.load(outfile)\n",
    "print(summaries[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "9e105282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pandemic (from Greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. A widespread endemic disease with a stable number of infected people is not a pandemic. Widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\\nThroughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. The most fatal pandemic in recorded history was the Black Death (also known as The Plague), which killed an estimated 75–200 million people in the 14th century. The term was not used yet but was for later pandemics including the 1918 influenza pandemic (Spanish flu). Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "036cb9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39465758",
   "metadata": {},
   "source": [
    "## Create a Python function that takes in a text string, performs all operations described in the previous step, and outputs a list of tokens (lemmas).\n",
    "\n",
    "- Lowercases the text string.\n",
    "\n",
    "\n",
    "- Creates a spaCy document with the text lemmas and their attributes using a spaCy model of your choice.\n",
    "\n",
    "\n",
    "- Removes stop words, punctuation, and other unclassified lemmas.\n",
    "\n",
    "\n",
    "- Returns a list of tokens (lemmas) found in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "09f12ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> a DET det a\n",
      "<class 'spacy.tokens.token.Token'> pandemic ADJ nsubj pandemic\n",
      "<class 'spacy.tokens.token.Token'> ( PUNCT punct (\n",
      "<class 'spacy.tokens.token.Token'> from ADP prep from\n",
      "<class 'spacy.tokens.token.Token'> greek ADJ amod greek\n"
     ]
    }
   ],
   "source": [
    "# Lowercase data. Lowercase the text\n",
    "# Explore the attributes of each token returned SpaCy.\n",
    "text = summaries[0]['text']\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized[:5]:\n",
    "    print(type(token), token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0dba0f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a pandemic (from greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. a widespread endemic disease with a stable number of infected people is not a pandemic. widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n",
       "throughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. the most fatal pandemic in recorded history was the black death (also known as the plague), which killed an estimated 75–200 million people in the 14th century. the term was not used yet but was for later pandemics including the 1918 influenza pandemic (spanish flu). current pandemics include covid-19 (sars-cov-2) and hiv/aids."
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "86927622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return sp(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "88c7809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a pandemic (from greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. a widespread endemic disease with a stable number of infected people is not a pandemic. widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n",
       "throughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. the most fatal pandemic in recorded history was the black death (also known as the plague), which killed an estimated 75–200 million people in the 14th century. the term was not used yet but was for later pandemics including the 1918 influenza pandemic (spanish flu). current pandemics include covid-19 (sars-cov-2) and hiv/aids."
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower(summaries[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8c0efc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation\n",
    "def remove_redundant_tokens(text_tokenized):\n",
    "    return ' '.join([token.text for token in text_tokenized if not token.is_stop and not token.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4f5c399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize (tokenize) the texts\n",
    "def lemmatize(text):\n",
    "    return [token.lemma_ for token in sp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a0a0a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tokenizer function\n",
    "def tokenizer(document):\n",
    "    \"\"\"\n",
    "    This function accepts a text string and:\n",
    "    1. Lowercases it\n",
    "    2. Removes redundant tokens\n",
    "    3. Performs token lemmatization\n",
    "    \"\"\" \n",
    "    text_tokenized = lower(document)\n",
    "    clean_text =  remove_redundant_tokens(text_tokenized)\n",
    "    token_lemmatized = lemmatize(clean_text)\n",
    "    return token_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334f8bc",
   "metadata": {},
   "source": [
    "## Use this function to preprocess all text documents in the dataset (text field only), and add the resulting lists to the dictionaries from step 1. \n",
    "\n",
    "You should end up with a list of dictionaries, each of which now has four key-value pairs:\n",
    "\n",
    "- title: Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- text: Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- tokenized_text: Tokenized Wikipedia article text.\n",
    "\n",
    "\n",
    "- url: Link to the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4f96b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all the documents using the tokenizer function\n",
    "for doc in summaries:\n",
    "    doc['tokenized_text'] = tokenizer(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "77c3aed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pandemic',\n",
       " 'greek',\n",
       " 'πᾶν',\n",
       " 'pan',\n",
       " 'δῆμος',\n",
       " 'demos',\n",
       " 'people',\n",
       " 'epidemic',\n",
       " 'infectious',\n",
       " 'disease']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]['tokenized_text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "69d4d918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries[0]['tokenized_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9785ef",
   "metadata": {},
   "source": [
    "## Save the new list of dictionaries in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6c97cf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='data/summaries.json' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenized texts to file:\n",
    "with open('data/summaries.json', 'w') as outfile:\n",
    "    json.dump(summaries, outfile)\n",
    "outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c22338",
   "metadata": {},
   "source": [
    "# M2 TF-IDF Search Using Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab2c60",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Implement a basic Tf-Idf search.\n",
    "\n",
    "- In your search for an optimal document retrieval method in the CDC’s huge knowledge base, you decide to try the term frequency search first because of its simplicity. It is a well-developed technique and is a great place to start!\n",
    "\n",
    "\n",
    "- In Milestone 1, you prepared the documents for Tf-Idf-based search. You also computed the Tf-Idf vectors for every document in the CDC’s knowledge base. The standard approach to finding the most relevant documents to your query is to compute similarities between the Tf-Idf vectors of the documents and the query. It works, but you realize that it can be very inefficient for very large document sets since you need to compute the similarities between the query and every one of the documents in your database. What would be a better solution? Let us move on to the last milestone of the project to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60074fe",
   "metadata": {},
   "source": [
    "## Load all relevant Python libraries and a spaCy language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9be9df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880bcc3",
   "metadata": {},
   "source": [
    "## Access the tokenized text in your new dataset from the previous milestone. \n",
    "\n",
    "Each document dictionary should now include a new key-value pair with the lemmatized text of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "2e071283",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/summaries.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f78980",
   "metadata": {},
   "source": [
    "## Create a corpus vocabulary. It should simply be a list of unique tokens in the provided set of documents. \n",
    "\n",
    "Count how many times each unique token appears in the corpus, you will need these counts for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "9e380e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n",
      "(3617,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1506"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all tokenized texts into a single list\n",
    "tokenized_texts = [i[\"tokenized_text\"] for i in data]\n",
    "print(np.array(tokenized_texts).shape)\n",
    "\n",
    "# flatten the list of lists (use itertools.chain)\n",
    "flattened_tokenized_texts = list(itertools.chain(*tokenized_texts))\n",
    "print(np.array(flattened_tokenized_texts).shape)\n",
    "\n",
    "# remove duplicates\n",
    "vocab = list(set(flattened_tokenized_texts))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "e50bf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary as a json file\n",
    "with open('data/vocab.json', 'w') as outfile:\n",
    "    json.dump(vocab, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "5b72e9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pandemic',\n",
       " 'greek',\n",
       " 'πᾶν',\n",
       " 'pan',\n",
       " 'δῆμος',\n",
       " 'demos',\n",
       " 'people',\n",
       " 'epidemic',\n",
       " 'infectious',\n",
       " 'disease']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['tokenized_text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "0926d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times each token occurs in a document\n",
    "# you will need it for TfIdf calculations\n",
    "docs_token_counter = []\n",
    "for doc in data:\n",
    "    # For each document, count how many of each token they have\n",
    "    # Counter function from collections is very handy\n",
    "    docs_token_counter.append([y for x, y in Counter(doc['tokenized_text']).items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "32e82f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [17,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  8,\n",
       "  8,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  6,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  12,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1],\n",
       " [1,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [1,\n",
       "  6,\n",
       "  5,\n",
       "  9,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  11,\n",
       "  17,\n",
       "  13,\n",
       "  11,\n",
       "  11,\n",
       "  14,\n",
       "  9,\n",
       "  9,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  10,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [4,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [8,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [3,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [5,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [3,\n",
       "  3,\n",
       "  4,\n",
       "  6,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [3,\n",
       "  3,\n",
       "  4,\n",
       "  9,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  8,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [4,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [7,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [8,\n",
       "  7,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [4,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [4,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  11,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [7,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [4,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  9,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [11,\n",
       "  11,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  7,\n",
       "  1,\n",
       "  10,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [14,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [19,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "50a18d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "361b99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_token_counter = []\n",
    "for doc in data:\n",
    "    doc_tokenized = doc[\"tokenized_text\"]\n",
    "    docs_token_counter.append(Counter(doc_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "47c0abcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'pandemic': 7,\n",
       "          'greek': 1,\n",
       "          'πᾶν': 1,\n",
       "          'pan': 1,\n",
       "          'δῆμος': 1,\n",
       "          'demos': 1,\n",
       "          'people': 5,\n",
       "          'epidemic': 1,\n",
       "          'infectious': 1,\n",
       "          'disease': 4,\n",
       "          'spread': 2,\n",
       "          'large': 2,\n",
       "          'region': 2,\n",
       "          'instance': 1,\n",
       "          'multiple': 1,\n",
       "          'continent': 1,\n",
       "          'worldwide': 2,\n",
       "          'affect': 1,\n",
       "          'substantial': 1,\n",
       "          'number': 4,\n",
       "          'widespread': 2,\n",
       "          'endemic': 2,\n",
       "          'stable': 2,\n",
       "          'infect': 2,\n",
       "          'recurrence': 1,\n",
       "          'seasonal': 1,\n",
       "          'influenza': 2,\n",
       "          'generally': 1,\n",
       "          'exclude': 1,\n",
       "          'occur': 1,\n",
       "          'simultaneously': 1,\n",
       "          'globe': 1,\n",
       "          '\\n ': 1,\n",
       "          'human': 1,\n",
       "          'history': 2,\n",
       "          'smallpox': 1,\n",
       "          'tuberculosis': 1,\n",
       "          'fatal': 1,\n",
       "          'record': 1,\n",
       "          'black': 1,\n",
       "          'death': 1,\n",
       "          'know': 1,\n",
       "          'plague': 1,\n",
       "          'kill': 1,\n",
       "          'estimate': 1,\n",
       "          '75–200': 1,\n",
       "          'million': 1,\n",
       "          '14th': 1,\n",
       "          'century': 1,\n",
       "          'term': 1,\n",
       "          'later': 1,\n",
       "          'include': 2,\n",
       "          '1918': 1,\n",
       "          'spanish': 1,\n",
       "          'flu': 1,\n",
       "          'current': 1,\n",
       "          'covid-19': 1,\n",
       "          'sar': 1,\n",
       "          'cov-2': 1,\n",
       "          'hiv': 1,\n",
       "          'aid': 1}),\n",
       " Counter({'hiv': 17,\n",
       "          'aid': 6,\n",
       "          'human': 1,\n",
       "          'immunodeficiency': 1,\n",
       "          'virus': 2,\n",
       "          'consider': 1,\n",
       "          'author': 1,\n",
       "          'global': 6,\n",
       "          'pandemic': 2,\n",
       "          'currently': 1,\n",
       "          'use': 1,\n",
       "          'term': 1,\n",
       "          'epidemic': 1,\n",
       "          'describe': 1,\n",
       "          '2018': 2,\n",
       "          'approximately': 3,\n",
       "          '37.9': 1,\n",
       "          'million': 8,\n",
       "          'people': 8,\n",
       "          'infect': 4,\n",
       "          'globally.there': 1,\n",
       "          '770,000': 1,\n",
       "          'death': 4,\n",
       "          '2018.the': 1,\n",
       "          '2015': 1,\n",
       "          'burden': 1,\n",
       "          'disease': 1,\n",
       "          'study': 1,\n",
       "          'report': 2,\n",
       "          'publish': 1,\n",
       "          'lancet': 1,\n",
       "          'estimate': 3,\n",
       "          'incidence': 5,\n",
       "          'infection': 4,\n",
       "          'peak': 1,\n",
       "          '1997': 2,\n",
       "          '3.3': 1,\n",
       "          'year': 3,\n",
       "          'fall': 2,\n",
       "          'rapidly': 1,\n",
       "          '2005': 2,\n",
       "          '2.6': 1,\n",
       "          'remain': 1,\n",
       "          'stable': 1,\n",
       "          '2015.sub': 1,\n",
       "          'saharan': 1,\n",
       "          'africa': 5,\n",
       "          'region': 3,\n",
       "          'affect': 1,\n",
       "          '61': 1,\n",
       "          'new': 2,\n",
       "          'occur': 1,\n",
       "          'prevalence': 6,\n",
       "          'ratio': 4,\n",
       "          'western': 3,\n",
       "          'central': 4,\n",
       "          'europe': 3,\n",
       "          'north': 2,\n",
       "          'america': 2,\n",
       "          'low': 2,\n",
       "          'decline': 1,\n",
       "          'mortality': 2,\n",
       "          '17': 1,\n",
       "          'see': 1,\n",
       "          '0.06': 3,\n",
       "          '2000': 2,\n",
       "          '0.03': 1,\n",
       "          '2017': 12,\n",
       "          'strong': 1,\n",
       "          'steady': 1,\n",
       "          'reduction': 1,\n",
       "          'eastern': 2,\n",
       "          'southern': 1,\n",
       "          'push': 1,\n",
       "          '0.11': 1,\n",
       "          '0.04': 1,\n",
       "          'progress': 1,\n",
       "          'gradual': 1,\n",
       "          'asia': 4,\n",
       "          'pacific': 1,\n",
       "          '0.05': 2,\n",
       "          'latin': 1,\n",
       "          'caribbean': 1,\n",
       "          'middle': 1,\n",
       "          'east': 3,\n",
       "          '0.08': 1,\n",
       "          '0.09': 1,\n",
       "          'south': 2,\n",
       "          'large': 1,\n",
       "          'population': 3,\n",
       "          'country': 1,\n",
       "          'world': 1,\n",
       "          '7.06': 1,\n",
       "          '  ': 1,\n",
       "          'tanzania': 1,\n",
       "          '4.5': 1,\n",
       "          'tanzanian': 1,\n",
       "          'adult': 1,\n",
       "          'age': 1,\n",
       "          '15–49': 1,\n",
       "          '2017.south': 1,\n",
       "          '2': 1,\n",
       "          'billion': 1,\n",
       "          '2010': 2,\n",
       "          '30': 2,\n",
       "          '4': 1,\n",
       "          'case': 3,\n",
       "          '12': 1,\n",
       "          '250,000': 1,\n",
       "          '2.5': 1,\n",
       "          'india': 1,\n",
       "          '0.3': 1,\n",
       "          'somewhat': 1,\n",
       "          'high': 1,\n",
       "          'find': 1,\n",
       "          'canada': 2,\n",
       "          '0.1%.in': 1,\n",
       "          '1': 1,\n",
       "          'unite': 1,\n",
       "          'state': 1,\n",
       "          '14': 1,\n",
       "          'realize': 1,\n",
       "          'infected.in': 1,\n",
       "          '93,385': 1,\n",
       "          '64,472': 1,\n",
       "          'man': 1,\n",
       "          '28,877': 1,\n",
       "          'woman': 1,\n",
       "          'live': 1,\n",
       "          'diagnose': 1,\n",
       "          'receive': 1,\n",
       "          'care': 1,\n",
       "          'uk': 1,\n",
       "          '428': 1,\n",
       "          '42,739': 1,\n",
       "          'nearly': 2,\n",
       "          '50': 1,\n",
       "          'gay': 1,\n",
       "          'bisexual': 1,\n",
       "          'small': 1,\n",
       "          'segment': 1,\n",
       "          'overall': 1,\n",
       "          '\\n ': 2,\n",
       "          'australia': 1,\n",
       "          '27,545': 1,\n",
       "          '2016': 1,\n",
       "          '63,110': 1,\n",
       "          'cases.a': 1,\n",
       "          'reconstruction': 1,\n",
       "          'genetic': 1,\n",
       "          'history': 1,\n",
       "          'show': 1,\n",
       "          'certainly': 1,\n",
       "          'originate': 1,\n",
       "          'kinshasa': 1,\n",
       "          'capital': 1,\n",
       "          'democratic': 1,\n",
       "          'republic': 1,\n",
       "          'congo': 1,\n",
       "          '1920': 1,\n",
       "          'recognize': 1,\n",
       "          '1981': 1,\n",
       "          '1983': 1,\n",
       "          'discover': 1,\n",
       "          'identify': 1,\n",
       "          'cause': 2,\n",
       "          '2009': 1}),\n",
       " Counter({'antonine': 1,\n",
       "          'plague': 8,\n",
       "          '165': 1,\n",
       "          '180': 1,\n",
       "          'ad': 2,\n",
       "          'know': 1,\n",
       "          'galen': 2,\n",
       "          'physician': 1,\n",
       "          'describe': 1,\n",
       "          'ancient': 2,\n",
       "          'pandemic': 2,\n",
       "          'bring': 1,\n",
       "          'roman': 7,\n",
       "          'empire': 2,\n",
       "          'troop': 1,\n",
       "          'return': 1,\n",
       "          'campaign': 1,\n",
       "          'near': 1,\n",
       "          'east': 1,\n",
       "          'scholar': 1,\n",
       "          'suspect': 1,\n",
       "          'smallpox': 1,\n",
       "          'measle': 1,\n",
       "          'claim': 1,\n",
       "          'life': 1,\n",
       "          'emperor': 1,\n",
       "          'lucius': 1,\n",
       "          'verus': 1,\n",
       "          'die': 2,\n",
       "          '169': 1,\n",
       "          'co': 1,\n",
       "          'regent': 1,\n",
       "          'marcus': 1,\n",
       "          'aurelius': 1,\n",
       "          'antoninus': 2,\n",
       "          'family': 1,\n",
       "          'associate': 1,\n",
       "          '\\n ': 1,\n",
       "          'source': 1,\n",
       "          'agree': 1,\n",
       "          'appear': 1,\n",
       "          'siege': 1,\n",
       "          'mesopotamian': 1,\n",
       "          'city': 1,\n",
       "          'seleucia': 1,\n",
       "          'winter': 1,\n",
       "          '165–166': 1,\n",
       "          'ammianus': 1,\n",
       "          'marcellinus': 1,\n",
       "          'report': 1,\n",
       "          'spread': 1,\n",
       "          'gaul': 1,\n",
       "          'legion': 1,\n",
       "          'rhine': 1,\n",
       "          'eutropius': 1,\n",
       "          'state': 1,\n",
       "          'large': 1,\n",
       "          'population': 2,\n",
       "          'accord': 1,\n",
       "          'contemporary': 1,\n",
       "          'historian': 2,\n",
       "          'cassius': 1,\n",
       "          'dio': 1,\n",
       "          'disease': 2,\n",
       "          'break': 2,\n",
       "          'year': 1,\n",
       "          'later': 1,\n",
       "          '189': 1,\n",
       "          'cause': 1,\n",
       "          '2,000': 1,\n",
       "          'death': 2,\n",
       "          'day': 1,\n",
       "          'rome': 1,\n",
       "          'quarter': 1,\n",
       "          'affect': 3,\n",
       "          'total': 1,\n",
       "          'count': 1,\n",
       "          'estimate': 1,\n",
       "          '5': 1,\n",
       "          'million': 1,\n",
       "          'kill': 1,\n",
       "          'area': 1,\n",
       "          'devastate': 1,\n",
       "          'army.australian': 1,\n",
       "          'sinologist': 1,\n",
       "          'rafe': 1,\n",
       "          'de': 1,\n",
       "          'crespigny': 1,\n",
       "          'speculate': 1,\n",
       "          'eastern': 1,\n",
       "          'han': 1,\n",
       "          'china': 1,\n",
       "          '166': 1,\n",
       "          'notice': 1,\n",
       "          'chinese': 1,\n",
       "          'record': 1,\n",
       "          'culture': 1,\n",
       "          'literature': 1,\n",
       "          'severely': 1,\n",
       "          'indo': 1,\n",
       "          'trade': 1,\n",
       "          'relation': 1,\n",
       "          'indian': 1,\n",
       "          'ocean': 1}),\n",
       " Counter({'epidemiology': 1,\n",
       "          'basic': 6,\n",
       "          'reproduction': 5,\n",
       "          'number': 9,\n",
       "          'reproductive': 2,\n",
       "          'call': 1,\n",
       "          'ratio': 1,\n",
       "          'rate': 2,\n",
       "          'denote': 1,\n",
       "          '\\n  \\n    \\n      \\n        \\n           ': 11,\n",
       "          'r': 17,\n",
       "          '\\n          \\n             ': 13,\n",
       "          '0': 11,\n",
       "          '\\n          \\n        \\n      \\n    \\n     ': 11,\n",
       "          '\\\\displaystyle': 14,\n",
       "          'r_{0': 9,\n",
       "          '\\n    ': 9,\n",
       "          'pronounce': 1,\n",
       "          'nought': 1,\n",
       "          'zero': 1,\n",
       "          'infection': 8,\n",
       "          'think': 1,\n",
       "          'expect': 1,\n",
       "          'case': 3,\n",
       "          'directly': 1,\n",
       "          'generate': 2,\n",
       "          'population': 10,\n",
       "          'individual': 2,\n",
       "          'susceptible': 4,\n",
       "          'definition': 2,\n",
       "          'describe': 1,\n",
       "          'state': 3,\n",
       "          'infect': 3,\n",
       "          'immunize': 3,\n",
       "          'naturally': 1,\n",
       "          'vaccination': 2,\n",
       "          'australian': 1,\n",
       "          'department': 1,\n",
       "          'health': 1,\n",
       "          'add': 1,\n",
       "          'absence': 1,\n",
       "          'deliberate': 1,\n",
       "          'intervention': 1,\n",
       "          'disease': 3,\n",
       "          'transmission': 1,\n",
       "          'confuse': 1,\n",
       "          'effective': 1,\n",
       "          '\\n  \\n    \\n      \\n         ': 3,\n",
       "          '\\n      \\n    \\n     ': 3,\n",
       "          'usually': 2,\n",
       "          'write': 1,\n",
       "          't': 2,\n",
       "          'r_{t': 1,\n",
       "          'time': 3,\n",
       "          'e': 1,\n",
       "          'r_{e': 1,\n",
       "          '\\n   ': 4,\n",
       "          'current': 1,\n",
       "          'uninfected': 1,\n",
       "          'important': 2,\n",
       "          'note': 1,\n",
       "          'dimensionless': 1,\n",
       "          'unit': 2,\n",
       "          'time−1': 1,\n",
       "          'like': 1,\n",
       "          'double': 1,\n",
       "          'biological': 1,\n",
       "          'constant': 1,\n",
       "          'pathogen': 1,\n",
       "          'affect': 3,\n",
       "          'factor': 2,\n",
       "          'environmental': 1,\n",
       "          'condition': 1,\n",
       "          'behaviour': 1,\n",
       "          'furthermore': 1,\n",
       "          'value': 7,\n",
       "          'estimate': 3,\n",
       "          'mathematical': 1,\n",
       "          'model': 5,\n",
       "          'dependent': 1,\n",
       "          'parameter': 1,\n",
       "          'give': 2,\n",
       "          'literature': 1,\n",
       "          'sense': 1,\n",
       "          'context': 1,\n",
       "          'recommend': 1,\n",
       "          'use': 2,\n",
       "          'obsolete': 1,\n",
       "          'compare': 1,\n",
       "          'base': 1,\n",
       "          'different': 1,\n",
       "          'fast': 1,\n",
       "          'spread': 4,\n",
       "          '\\n ': 1,\n",
       "          'determine': 2,\n",
       "          'emerge': 1,\n",
       "          'infectious': 1,\n",
       "          'proportion': 3,\n",
       "          'eradicate': 1,\n",
       "          'commonly': 1,\n",
       "          '\\n          \\n        \\n         ': 2,\n",
       "          '>': 1,\n",
       "          '\\n         ': 4,\n",
       "          '1': 8,\n",
       "          'r_{0}>1': 1,\n",
       "          'able': 1,\n",
       "          'start': 1,\n",
       "          '<': 1,\n",
       "          'r_{0}<1': 1,\n",
       "          'generally': 1,\n",
       "          'large': 2,\n",
       "          'hard': 1,\n",
       "          'control': 1,\n",
       "          'epidemic': 1,\n",
       "          'simple': 1,\n",
       "          'need': 1,\n",
       "          'effectively': 1,\n",
       "          'mean': 1,\n",
       "          'prevent': 1,\n",
       "          'sustained': 1,\n",
       "          '−': 1,\n",
       "          '\\n        \\n           \\n        \\n        \\n           ': 2,\n",
       "          'conversely': 1,\n",
       "          'remain': 1,\n",
       "          'endemic': 1,\n",
       "          'equilibrium': 1,\n",
       "          '\\n   \\n ': 1,\n",
       "          'include': 1,\n",
       "          'duration': 1,\n",
       "          'infectivity': 1,\n",
       "          'people': 3,\n",
       "          'infectiousness': 1,\n",
       "          'microorganism': 1,\n",
       "          'contact': 1}),\n",
       " Counter({'bill': 4,\n",
       "          'mortality': 2,\n",
       "          'weekly': 3,\n",
       "          'statistic': 2,\n",
       "          'london': 5,\n",
       "          'design': 1,\n",
       "          'monitor': 1,\n",
       "          'burial': 2,\n",
       "          '1592': 1,\n",
       "          '1595': 1,\n",
       "          'continuously': 1,\n",
       "          '1603': 1,\n",
       "          'responsibility': 1,\n",
       "          'produce': 1,\n",
       "          'charter': 1,\n",
       "          '1611': 1,\n",
       "          'worshipful': 1,\n",
       "          'company': 1,\n",
       "          'parish': 5,\n",
       "          'clerk': 1,\n",
       "          'cover': 1,\n",
       "          'area': 5,\n",
       "          'start': 1,\n",
       "          'expand': 1,\n",
       "          'grow': 1,\n",
       "          'city': 1,\n",
       "          'reach': 1,\n",
       "          'maximum': 1,\n",
       "          'extent': 1,\n",
       "          '1636': 1,\n",
       "          'new': 1,\n",
       "          'add': 1,\n",
       "          'ancient': 1,\n",
       "          'divide': 1,\n",
       "          'factor': 1,\n",
       "          'use': 1,\n",
       "          'suburban': 1,\n",
       "          'cemetery': 1,\n",
       "          'outside': 1,\n",
       "          'exemption': 1,\n",
       "          'extra': 1,\n",
       "          'parochial': 1,\n",
       "          'place': 1,\n",
       "          'wide': 1,\n",
       "          'growth': 1,\n",
       "          'metropolis': 1,\n",
       "          'record': 1,\n",
       "          'death': 1,\n",
       "          'render': 1,\n",
       "          'datum': 1,\n",
       "          'incomplete': 1,\n",
       "          'production': 1,\n",
       "          'go': 1,\n",
       "          'decline': 1,\n",
       "          '1819': 1,\n",
       "          'cease': 1,\n",
       "          'provide': 1,\n",
       "          'return': 2,\n",
       "          'survive': 1,\n",
       "          'date': 1,\n",
       "          '1858': 1,\n",
       "          'supersede': 1,\n",
       "          'registrar': 1,\n",
       "          'general': 1,\n",
       "          '1840': 1,\n",
       "          'take': 1,\n",
       "          '1847': 1,\n",
       "          'district': 1,\n",
       "          'metropolitan': 1,\n",
       "          'board': 1,\n",
       "          'work': 1,\n",
       "          '1855': 1,\n",
       "          'county': 1,\n",
       "          '1889': 1,\n",
       "          'inner': 1,\n",
       "          '1965': 1}),\n",
       " Counter({'cholera': 8,\n",
       "          'infection': 1,\n",
       "          'small': 1,\n",
       "          'intestine': 1,\n",
       "          'strain': 1,\n",
       "          'bacterium': 1,\n",
       "          'vibrio': 2,\n",
       "          'cholerae': 2,\n",
       "          'symptom': 3,\n",
       "          'range': 1,\n",
       "          'mild': 1,\n",
       "          'severe': 5,\n",
       "          'classic': 1,\n",
       "          'large': 2,\n",
       "          'amount': 1,\n",
       "          'watery': 1,\n",
       "          'diarrhea': 3,\n",
       "          'last': 1,\n",
       "          'day': 2,\n",
       "          'vomit': 1,\n",
       "          'muscle': 1,\n",
       "          'cramp': 1,\n",
       "          'occur': 3,\n",
       "          'lead': 2,\n",
       "          'hour': 2,\n",
       "          'dehydration': 2,\n",
       "          'electrolyte': 1,\n",
       "          'imbalance': 1,\n",
       "          'result': 2,\n",
       "          'sink': 1,\n",
       "          'eye': 1,\n",
       "          'cold': 1,\n",
       "          'skin': 3,\n",
       "          'decrease': 1,\n",
       "          'elasticity': 1,\n",
       "          'wrinkling': 1,\n",
       "          'hand': 1,\n",
       "          'foot': 1,\n",
       "          'cause': 4,\n",
       "          'turn': 1,\n",
       "          'bluish': 1,\n",
       "          'start': 1,\n",
       "          'exposure.cholera': 1,\n",
       "          'number': 1,\n",
       "          'type': 3,\n",
       "          'produce': 1,\n",
       "          'disease': 4,\n",
       "          'spread': 1,\n",
       "          'unsafe': 2,\n",
       "          'water': 3,\n",
       "          'food': 1,\n",
       "          'contaminate': 1,\n",
       "          'human': 2,\n",
       "          'fece': 1,\n",
       "          'contain': 1,\n",
       "          'bacteria': 1,\n",
       "          'undercooked': 1,\n",
       "          'seafood': 1,\n",
       "          'common': 1,\n",
       "          'source': 1,\n",
       "          'animal': 1,\n",
       "          'affect': 4,\n",
       "          'risk': 3,\n",
       "          'factor': 1,\n",
       "          'include': 3,\n",
       "          'poor': 1,\n",
       "          'sanitation': 2,\n",
       "          'clean': 2,\n",
       "          'drinking': 1,\n",
       "          'poverty': 1,\n",
       "          'concern': 1,\n",
       "          'rise': 1,\n",
       "          'sea': 1,\n",
       "          'level': 1,\n",
       "          'increase': 1,\n",
       "          'rate': 2,\n",
       "          'diagnose': 1,\n",
       "          'stool': 1,\n",
       "          'test': 2,\n",
       "          'rapid': 1,\n",
       "          'dipstick': 1,\n",
       "          'available': 1,\n",
       "          'accurate.prevention': 1,\n",
       "          'method': 1,\n",
       "          'improve': 1,\n",
       "          'access': 2,\n",
       "          'vaccine': 1,\n",
       "          'give': 1,\n",
       "          'mouth': 1,\n",
       "          'provide': 1,\n",
       "          'reasonable': 1,\n",
       "          'protection': 1,\n",
       "          'month': 1,\n",
       "          'add': 1,\n",
       "          'benefit': 1,\n",
       "          'protect': 1,\n",
       "          'e.': 1,\n",
       "          'coli': 1,\n",
       "          'primary': 1,\n",
       "          'treatment': 2,\n",
       "          'oral': 1,\n",
       "          'rehydration': 1,\n",
       "          'therapy': 1,\n",
       "          'replacement': 1,\n",
       "          'fluid': 2,\n",
       "          'slightly': 1,\n",
       "          'sweet': 1,\n",
       "          'salty': 1,\n",
       "          'solution': 2,\n",
       "          'rice': 1,\n",
       "          'base': 1,\n",
       "          'prefer': 1,\n",
       "          'zinc': 1,\n",
       "          'supplementation': 1,\n",
       "          'useful': 1,\n",
       "          'child': 2,\n",
       "          'case': 1,\n",
       "          'intravenous': 1,\n",
       "          'ringer': 1,\n",
       "          'lactate': 1,\n",
       "          'require': 1,\n",
       "          'antibiotic': 2,\n",
       "          'beneficial': 1,\n",
       "          'testing': 1,\n",
       "          'susceptible': 1,\n",
       "          'help': 1,\n",
       "          'guide': 1,\n",
       "          'choice.cholera': 1,\n",
       "          'estimate': 1,\n",
       "          '3–5': 1,\n",
       "          'million': 2,\n",
       "          'people': 1,\n",
       "          'worldwide': 1,\n",
       "          '28,800–130,000': 1,\n",
       "          'death': 4,\n",
       "          'year': 2,\n",
       "          'classify': 1,\n",
       "          'pandemic': 1,\n",
       "          '2010': 1,\n",
       "          'rare': 1,\n",
       "          'develop': 1,\n",
       "          'world': 1,\n",
       "          'outbreak': 2,\n",
       "          'chronically': 1,\n",
       "          'certain': 1,\n",
       "          'area': 2,\n",
       "          'ongoing': 1,\n",
       "          'africa': 1,\n",
       "          'southeast': 1,\n",
       "          'asia': 1,\n",
       "          'usually': 1,\n",
       "          '5': 1,\n",
       "          'high': 2,\n",
       "          '50': 1,\n",
       "          'description': 1,\n",
       "          'find': 1,\n",
       "          'early': 1,\n",
       "          '5th': 1,\n",
       "          'century': 1,\n",
       "          'bc': 1,\n",
       "          'sanskrit': 1,\n",
       "          'study': 1,\n",
       "          'england': 1,\n",
       "          'john': 1,\n",
       "          'snow': 1,\n",
       "          '1849': 1,\n",
       "          '1854': 1,\n",
       "          'significant': 1,\n",
       "          'advance': 1,\n",
       "          'field': 1,\n",
       "          'epidemiology': 1,\n",
       "          'seven': 1,\n",
       "          '200': 1}),\n",
       " Counter({'covid-19': 3,\n",
       "          'pandemic': 4,\n",
       "          'know': 2,\n",
       "          'coronavirus': 3,\n",
       "          'ongoing': 1,\n",
       "          'disease': 2,\n",
       "          '2019': 2,\n",
       "          'cause': 2,\n",
       "          'severe': 1,\n",
       "          'acute': 2,\n",
       "          'respiratory': 2,\n",
       "          'syndrome': 2,\n",
       "          '2': 1,\n",
       "          'sar': 1,\n",
       "          'cov-2': 1,\n",
       "          'identify': 1,\n",
       "          'december': 1,\n",
       "          'wuhan': 1,\n",
       "          'china': 1,\n",
       "          'outbreak': 1,\n",
       "          'declare': 1,\n",
       "          'public': 1,\n",
       "          'health': 1,\n",
       "          'emergency': 1,\n",
       "          'international': 1,\n",
       "          'concern': 1,\n",
       "          'january': 1,\n",
       "          '2020': 3,\n",
       "          'march': 1,\n",
       "          '17': 1,\n",
       "          'october': 1,\n",
       "          '39.5': 1,\n",
       "          'million': 3,\n",
       "          'case': 1,\n",
       "          'confirm': 1,\n",
       "          '1.1': 1,\n",
       "          'death': 1,\n",
       "          'attribute': 1,\n",
       "          '\\n\\n ': 1,\n",
       "          'common': 1,\n",
       "          'symptom': 1,\n",
       "          'include': 4,\n",
       "          'fever': 1,\n",
       "          'cough': 2,\n",
       "          'fatigue': 1,\n",
       "          'breathing': 1,\n",
       "          'difficulty': 1,\n",
       "          'loss': 1,\n",
       "          'smell': 1,\n",
       "          'complication': 1,\n",
       "          'pneumonia': 1,\n",
       "          'distress': 1,\n",
       "          'incubation': 1,\n",
       "          'period': 1,\n",
       "          'typically': 1,\n",
       "          'day': 2,\n",
       "          'range': 1,\n",
       "          '14': 1,\n",
       "          'vaccine': 1,\n",
       "          'candidate': 1,\n",
       "          'development': 1,\n",
       "          'prove': 1,\n",
       "          'safety': 1,\n",
       "          'efficacy': 1,\n",
       "          'specific': 1,\n",
       "          'antiviral': 1,\n",
       "          'medication': 1,\n",
       "          'primary': 1,\n",
       "          'treatment': 1,\n",
       "          'currently': 1,\n",
       "          'symptomatic': 2,\n",
       "          '\\n ': 1,\n",
       "          'recommend': 1,\n",
       "          'preventive': 1,\n",
       "          'measure': 1,\n",
       "          'hand': 1,\n",
       "          'washing': 1,\n",
       "          'cover': 1,\n",
       "          'mouth': 1,\n",
       "          'wear': 1,\n",
       "          'face': 1,\n",
       "          'mask': 1,\n",
       "          'sneeze': 1,\n",
       "          'social': 2,\n",
       "          'distancing': 1,\n",
       "          'disinfect': 1,\n",
       "          'surface': 1,\n",
       "          'ventilation': 1,\n",
       "          'air': 1,\n",
       "          'filtering': 1,\n",
       "          'monitoring': 1,\n",
       "          'self': 1,\n",
       "          'isolation': 1,\n",
       "          'expose': 1,\n",
       "          'travel': 1,\n",
       "          'restriction': 1,\n",
       "          'lockdown': 1,\n",
       "          'workplace': 1,\n",
       "          'hazard': 1,\n",
       "          'control': 1,\n",
       "          'facility': 1,\n",
       "          'closure': 1,\n",
       "          'implement': 1,\n",
       "          'place': 1,\n",
       "          'work': 1,\n",
       "          'increase': 1,\n",
       "          'testing': 1,\n",
       "          'capacity': 1,\n",
       "          'trace': 1,\n",
       "          'contact': 1,\n",
       "          'infect': 1,\n",
       "          'economic': 1,\n",
       "          'disruption': 1,\n",
       "          'large': 1,\n",
       "          'global': 2,\n",
       "          'recession': 1,\n",
       "          'great': 1,\n",
       "          'depression': 1,\n",
       "          'extreme': 1,\n",
       "          'poverty': 1,\n",
       "          'famine': 1,\n",
       "          'affect': 2,\n",
       "          'hundred': 1,\n",
       "          'inflame': 1,\n",
       "          'supply': 1,\n",
       "          'shortage': 1,\n",
       "          'event': 1,\n",
       "          'environment': 1,\n",
       "          'education': 1,\n",
       "          'system': 1,\n",
       "          'misinformation': 1,\n",
       "          'virus': 1,\n",
       "          'circulate': 1,\n",
       "          'globally': 1,\n",
       "          'incident': 1,\n",
       "          'xenophobia': 1,\n",
       "          'racism': 1,\n",
       "          'chinese': 2,\n",
       "          'people': 1,\n",
       "          'perceive': 1,\n",
       "          'area': 1,\n",
       "          'high': 1,\n",
       "          'infection': 1,\n",
       "          'rate': 1}),\n",
       " Counter({'crimson': 1,\n",
       "          'contagion': 1,\n",
       "          'joint': 1,\n",
       "          'exercise': 2,\n",
       "          'conduct': 2,\n",
       "          'january': 1,\n",
       "          'august': 1,\n",
       "          '2019': 1,\n",
       "          'numerous': 1,\n",
       "          'national': 1,\n",
       "          'state': 3,\n",
       "          'local': 1,\n",
       "          'private': 1,\n",
       "          'public': 1,\n",
       "          'organization': 1,\n",
       "          'participate': 1,\n",
       "          'order': 1,\n",
       "          'test': 1,\n",
       "          'capacity': 2,\n",
       "          'federal': 2,\n",
       "          'government': 2,\n",
       "          'respond': 2,\n",
       "          'severe': 1,\n",
       "          'pandemic': 3,\n",
       "          'influenza': 1,\n",
       "          'originate': 1,\n",
       "          'china': 2,\n",
       "          '\\n ': 1,\n",
       "          'simulation': 1,\n",
       "          'month': 2,\n",
       "          'prior': 1,\n",
       "          'start': 1,\n",
       "          'covid-19': 1,\n",
       "          'involve': 1,\n",
       "          'scenario': 1,\n",
       "          'tourist': 1,\n",
       "          'return': 1,\n",
       "          'spread': 1,\n",
       "          'respiratory': 1,\n",
       "          'virus': 3,\n",
       "          'united': 1,\n",
       "          'begin': 1,\n",
       "          'chicago': 1,\n",
       "          'infect': 1,\n",
       "          '110': 1,\n",
       "          'million': 2,\n",
       "          'american': 1,\n",
       "          'kill': 1,\n",
       "          'half': 1,\n",
       "          'report': 1,\n",
       "          'issue': 1,\n",
       "          'conclusion': 1,\n",
       "          'outline': 1,\n",
       "          'limit': 1,\n",
       "          'agency': 1,\n",
       "          'lack': 1,\n",
       "          'fund': 1,\n",
       "          'coordination': 1,\n",
       "          'resource': 1,\n",
       "          'facilitate': 1,\n",
       "          'effective': 1,\n",
       "          'response': 1}),\n",
       " Counter({'disease': 5,\n",
       "          'x': 2,\n",
       "          'placeholder': 2,\n",
       "          'adopt': 2,\n",
       "          'world': 1,\n",
       "          'health': 1,\n",
       "          'organization': 1,\n",
       "          'february': 1,\n",
       "          '2018': 1,\n",
       "          'shortlist': 1,\n",
       "          'blueprint': 1,\n",
       "          'priority': 1,\n",
       "          'represent': 1,\n",
       "          'hypothetical': 1,\n",
       "          'unknown': 2,\n",
       "          'pathogen': 2,\n",
       "          'cause': 2,\n",
       "          'future': 1,\n",
       "          'epidemic': 1,\n",
       "          'ensure': 1,\n",
       "          'plan': 1,\n",
       "          'sufficiently': 1,\n",
       "          'flexible': 1,\n",
       "          'adapt': 1,\n",
       "          'e.g.': 3,\n",
       "          'broad': 1,\n",
       "          'vaccine': 1,\n",
       "          'manufacturing': 1,\n",
       "          'facility': 1,\n",
       "          '  ': 2,\n",
       "          'director': 1,\n",
       "          'national': 1,\n",
       "          'institute': 1,\n",
       "          'allergy': 1,\n",
       "          'infectious': 1,\n",
       "          'anthony': 1,\n",
       "          'fauci': 1,\n",
       "          'state': 1,\n",
       "          'concept': 1,\n",
       "          'encourage': 1,\n",
       "          'project': 1,\n",
       "          'focus': 1,\n",
       "          'research': 1,\n",
       "          'effort': 1,\n",
       "          'entire': 1,\n",
       "          'class': 1,\n",
       "          'virus': 3,\n",
       "          'flaviviruse': 1,\n",
       "          'instead': 1,\n",
       "          'individual': 1,\n",
       "          'strain': 3,\n",
       "          'zika': 1,\n",
       "          'improve': 1,\n",
       "          'capability': 1,\n",
       "          'respond': 1,\n",
       "          'unforeseen': 1,\n",
       "          '2020': 1,\n",
       "          'speculate': 1,\n",
       "          'include': 1,\n",
       "          'expert': 1,\n",
       "          'advisor': 1,\n",
       "          'covid-19': 1,\n",
       "          'sar': 1,\n",
       "          'cov-2': 1,\n",
       "          'meet': 1,\n",
       "          'requirement': 1,\n",
       "          'x.': 1}),\n",
       " Counter({'johns': 3,\n",
       "          'hopkin': 3,\n",
       "          'center': 4,\n",
       "          'health': 6,\n",
       "          'security': 2,\n",
       "          'abbreviate': 1,\n",
       "          'chs': 1,\n",
       "          'previously': 1,\n",
       "          'upmc': 2,\n",
       "          'biosecurity': 2,\n",
       "          'civilian': 1,\n",
       "          'biodefense': 1,\n",
       "          'strategy': 1,\n",
       "          'independent': 1,\n",
       "          'nonprofit': 1,\n",
       "          'organization': 2,\n",
       "          'bloomberg': 1,\n",
       "          'school': 1,\n",
       "          'public': 1,\n",
       "          'environmental': 1,\n",
       "          'engineering': 1,\n",
       "          'department': 1,\n",
       "          'concern': 1,\n",
       "          'area': 1,\n",
       "          'consequence': 1,\n",
       "          'epidemic': 1,\n",
       "          'disaster': 1,\n",
       "          'avert': 1,\n",
       "          'biological': 2,\n",
       "          'weapon': 2,\n",
       "          'development': 1,\n",
       "          'implication': 1,\n",
       "          'bioeconomy': 1,\n",
       "          'think': 1,\n",
       "          'tank': 1,\n",
       "          'policy': 2,\n",
       "          'research': 1,\n",
       "          'give': 1,\n",
       "          'recommendation': 1,\n",
       "          'unite': 1,\n",
       "          'state': 1,\n",
       "          'government': 1,\n",
       "          'world': 1,\n",
       "          'un': 1,\n",
       "          'convention': 1}),\n",
       " Counter({'human': 3,\n",
       "          'immunodeficiency': 3,\n",
       "          'virus': 4,\n",
       "          'infection': 9,\n",
       "          'acquire': 2,\n",
       "          'immune': 3,\n",
       "          'deficiency': 1,\n",
       "          'syndrome': 2,\n",
       "          'hiv': 8,\n",
       "          'aid': 7,\n",
       "          'spectrum': 1,\n",
       "          'condition': 1,\n",
       "          'cause': 3,\n",
       "          'follow': 2,\n",
       "          'initial': 1,\n",
       "          'person': 1,\n",
       "          'notice': 1,\n",
       "          'symptom': 3,\n",
       "          'experience': 1,\n",
       "          'brief': 1,\n",
       "          'period': 2,\n",
       "          'influenza': 1,\n",
       "          'like': 1,\n",
       "          'illness': 2,\n",
       "          'typically': 1,\n",
       "          'prolonged': 1,\n",
       "          'progress': 1,\n",
       "          'interfere': 1,\n",
       "          'system': 1,\n",
       "          'increase': 1,\n",
       "          'risk': 1,\n",
       "          'develop': 1,\n",
       "          'common': 1,\n",
       "          'tuberculosis': 1,\n",
       "          'opportunistic': 1,\n",
       "          'tumor': 1,\n",
       "          'rare': 1,\n",
       "          'people': 2,\n",
       "          'normal': 2,\n",
       "          'function': 1,\n",
       "          'late': 1,\n",
       "          'refer': 1,\n",
       "          'stage': 1,\n",
       "          'associate': 1,\n",
       "          'unintended': 1,\n",
       "          'weight': 1,\n",
       "          'loss.hiv': 1,\n",
       "          'spread': 1,\n",
       "          'primarily': 1,\n",
       "          'unprotected': 1,\n",
       "          'sex': 3,\n",
       "          'include': 3,\n",
       "          'anal': 1,\n",
       "          'oral': 1,\n",
       "          'contaminate': 1,\n",
       "          'blood': 1,\n",
       "          'transfusion': 1,\n",
       "          'hypodermic': 1,\n",
       "          'needle': 2,\n",
       "          'mother': 2,\n",
       "          'child': 2,\n",
       "          'pregnancy': 1,\n",
       "          'delivery': 1,\n",
       "          'breastfeeding': 1,\n",
       "          'bodily': 1,\n",
       "          'fluid': 1,\n",
       "          'saliva': 1,\n",
       "          'sweat': 1,\n",
       "          'tear': 1,\n",
       "          'transmit': 2,\n",
       "          'member': 1,\n",
       "          'group': 1,\n",
       "          'know': 1,\n",
       "          'retroviruses.method': 1,\n",
       "          'prevention': 3,\n",
       "          'safe': 1,\n",
       "          'exchange': 1,\n",
       "          'program': 1,\n",
       "          'treat': 1,\n",
       "          'infected': 1,\n",
       "          'pre-': 1,\n",
       "          'post': 1,\n",
       "          'exposure': 1,\n",
       "          'prophylaxis': 1,\n",
       "          'disease': 7,\n",
       "          'baby': 1,\n",
       "          'prevent': 1,\n",
       "          'give': 1,\n",
       "          'antiretroviral': 2,\n",
       "          'medication': 1,\n",
       "          'cure': 1,\n",
       "          'vaccine': 1,\n",
       "          'treatment': 3,\n",
       "          'slow': 1,\n",
       "          'course': 1,\n",
       "          'lead': 1,\n",
       "          'near': 1,\n",
       "          'life': 1,\n",
       "          'expectancy': 1,\n",
       "          'recommend': 1,\n",
       "          'soon': 1,\n",
       "          'diagnosis': 1,\n",
       "          'average': 1,\n",
       "          'survival': 1,\n",
       "          'time': 2,\n",
       "          '11': 1,\n",
       "          'years.in': 1,\n",
       "          '2018': 2,\n",
       "          '37.9': 1,\n",
       "          'million': 3,\n",
       "          'live': 2,\n",
       "          'result': 1,\n",
       "          '770,000': 1,\n",
       "          'death': 2,\n",
       "          'estimate': 2,\n",
       "          '20.6': 1,\n",
       "          'eastern': 1,\n",
       "          'southern': 1,\n",
       "          'africa': 2,\n",
       "          'identify': 3,\n",
       "          'early': 3,\n",
       "          '1980': 2,\n",
       "          '32': 1,\n",
       "          'worldwide': 1,\n",
       "          'consider': 1,\n",
       "          'pandemic': 1,\n",
       "          'outbreak': 1,\n",
       "          'present': 1,\n",
       "          'large': 4,\n",
       "          'area': 1,\n",
       "          'actively': 1,\n",
       "          'spreading.hiv': 1,\n",
       "          'jump': 1,\n",
       "          'primate': 1,\n",
       "          'west': 1,\n",
       "          'central': 1,\n",
       "          'mid': 1,\n",
       "          '20th': 1,\n",
       "          'century': 1,\n",
       "          'recognize': 1,\n",
       "          'united': 1,\n",
       "          'state': 1,\n",
       "          'center': 1,\n",
       "          'control': 1,\n",
       "          'cdc': 1,\n",
       "          '1981': 1,\n",
       "          'decade': 1,\n",
       "          '\\n ': 1,\n",
       "          'impact': 2,\n",
       "          'society': 1,\n",
       "          'source': 1,\n",
       "          'discrimination': 1,\n",
       "          'economic': 1,\n",
       "          'misconception': 1,\n",
       "          'belief': 1,\n",
       "          'casual': 1,\n",
       "          'non': 1,\n",
       "          'sexual': 1,\n",
       "          'contact': 1,\n",
       "          'subject': 1,\n",
       "          'controversy': 1,\n",
       "          'involve': 1,\n",
       "          'religion': 1,\n",
       "          'catholic': 1,\n",
       "          'church': 1,\n",
       "          'position': 1,\n",
       "          'support': 1,\n",
       "          'condom': 1,\n",
       "          'use': 1,\n",
       "          'attract': 1,\n",
       "          'international': 1,\n",
       "          'medical': 1,\n",
       "          'political': 1,\n",
       "          'attention': 1,\n",
       "          'scale': 1,\n",
       "          'funding': 1}),\n",
       " Counter({'people': 4,\n",
       "          'republic': 1,\n",
       "          'china': 5,\n",
       "          'report': 3,\n",
       "          'aid': 3,\n",
       "          'case': 3,\n",
       "          'identify': 1,\n",
       "          '1985': 1,\n",
       "          'die': 1,\n",
       "          'tourist': 1,\n",
       "          '1989': 2,\n",
       "          'indigenous': 1,\n",
       "          'outbreak': 1,\n",
       "          '146': 1,\n",
       "          'infected': 1,\n",
       "          'heroin': 1,\n",
       "          'user': 3,\n",
       "          'yunnan': 1,\n",
       "          'province': 2,\n",
       "          'near': 2,\n",
       "          'southwest': 1,\n",
       "          'border.yunnan': 1,\n",
       "          'area': 2,\n",
       "          'affect': 1,\n",
       "          'hiv': 3,\n",
       "          'infection': 1,\n",
       "          'appear': 2,\n",
       "          'needle': 1,\n",
       "          'share': 1,\n",
       "          'drug': 2,\n",
       "          'burmese': 1,\n",
       "          'border': 2,\n",
       "          '1993': 1,\n",
       "          'disease': 2,\n",
       "          'remain': 1,\n",
       "          'problem': 1,\n",
       "          'mobile': 1,\n",
       "          'truck': 1,\n",
       "          'driver': 1,\n",
       "          'construction': 1,\n",
       "          'migrant': 1,\n",
       "          'worker': 3,\n",
       "          'traveler': 1,\n",
       "          'bring': 1,\n",
       "          'virus': 3,\n",
       "          'country': 2,\n",
       "          '1995': 1,\n",
       "          'sichuan': 1,\n",
       "          'xinjiang': 1,\n",
       "          '1998': 1,\n",
       "          'spread': 2,\n",
       "          '\\n ': 1,\n",
       "          'low': 1,\n",
       "          'awareness': 1,\n",
       "          'general': 2,\n",
       "          'population': 2,\n",
       "          'major': 1,\n",
       "          'culprit': 1,\n",
       "          'chinese': 1,\n",
       "          'consider': 1,\n",
       "          'foreign': 1,\n",
       "          'issue': 1,\n",
       "          'educate': 1,\n",
       "          'knowledgeable': 1,\n",
       "          'transmission': 1,\n",
       "          'prevention': 1,\n",
       "          'recently': 1,\n",
       "          'use': 1,\n",
       "          'condom': 1,\n",
       "          'common': 1,\n",
       "          'sex': 2,\n",
       "          'client': 1,\n",
       "          'result': 1,\n",
       "          'epidemic': 1,\n",
       "          'high': 1,\n",
       "          'risk': 1,\n",
       "          'group': 1,\n",
       "          'unsafe': 1,\n",
       "          'blood': 1,\n",
       "          'donor': 1}),\n",
       " Counter({'pandemic': 7,\n",
       "          'prevention': 2,\n",
       "          'organization': 1,\n",
       "          'management': 1,\n",
       "          'preventive': 1,\n",
       "          'measure': 3,\n",
       "          'include': 1,\n",
       "          'reduce': 1,\n",
       "          'cause': 1,\n",
       "          'new': 1,\n",
       "          'infectious': 1,\n",
       "          'disease': 1,\n",
       "          'prevent': 1,\n",
       "          'outbreak': 1,\n",
       "          'epidemic': 1,\n",
       "          '\\n ': 1,\n",
       "          'mistake': 1,\n",
       "          'preparedness': 1,\n",
       "          'mitigation': 1,\n",
       "          'largely': 1,\n",
       "          'seek': 1,\n",
       "          'mitigate': 1,\n",
       "          'magnitude': 1,\n",
       "          'negative': 1,\n",
       "          'effect': 1,\n",
       "          'overlap': 1,\n",
       "          'respect': 1}),\n",
       " Counter({'pandemic': 8,\n",
       "          'severity': 7,\n",
       "          'assessment': 3,\n",
       "          'framework': 2,\n",
       "          'psaf': 3,\n",
       "          'evaluation': 1,\n",
       "          'use': 1,\n",
       "          'quadrant': 1,\n",
       "          'evaluate': 1,\n",
       "          'transmissibility': 2,\n",
       "          'clinical': 2,\n",
       "          'combine': 1,\n",
       "          'overall': 1,\n",
       "          'impact': 1,\n",
       "          'estimate': 1,\n",
       "          '\\n ': 1,\n",
       "          'calculate': 1,\n",
       "          'multiple': 1,\n",
       "          'measure': 2,\n",
       "          'include': 1,\n",
       "          'case': 3,\n",
       "          'fatality': 2,\n",
       "          'rate': 7,\n",
       "          'hospitalization': 2,\n",
       "          'ratio': 2,\n",
       "          'death': 1,\n",
       "          'viral': 1,\n",
       "          'available': 1,\n",
       "          'datum': 1,\n",
       "          'secondary': 1,\n",
       "          'household': 1,\n",
       "          'attack': 4,\n",
       "          'school': 1,\n",
       "          'workplace': 1,\n",
       "          'community': 1,\n",
       "          'emergency': 1,\n",
       "          'department': 1,\n",
       "          'outpatient': 1,\n",
       "          'visit': 1,\n",
       "          'influenza': 1,\n",
       "          'like': 1,\n",
       "          'illness.the': 1,\n",
       "          'supersede': 1,\n",
       "          '2007': 1,\n",
       "          'linear': 1,\n",
       "          'index': 1,\n",
       "          'psi': 1,\n",
       "          'assume': 1,\n",
       "          '30': 1,\n",
       "          'spread': 1,\n",
       "          'measured': 1,\n",
       "          'cfr': 1,\n",
       "          'assess': 1,\n",
       "          'evolution': 1,\n",
       "          'united': 1,\n",
       "          'state': 1,\n",
       "          'center': 1,\n",
       "          'disease': 1,\n",
       "          'control': 1,\n",
       "          'prevention': 1,\n",
       "          'cdc': 2,\n",
       "          'adopt': 1,\n",
       "          'official': 2,\n",
       "          'tool': 2,\n",
       "          '2014': 1,\n",
       "          'list': 1,\n",
       "          'national': 1,\n",
       "          'strategy': 1,\n",
       "          'time': 1,\n",
       "          'covid-19': 1}),\n",
       " Counter({'pandemic': 4,\n",
       "          'severity': 4,\n",
       "          'index': 2,\n",
       "          'psi': 3,\n",
       "          'propose': 1,\n",
       "          'classification': 2,\n",
       "          'scale': 3,\n",
       "          'reporting': 1,\n",
       "          'influenza': 1,\n",
       "          'united': 2,\n",
       "          'state': 2,\n",
       "          'accompany': 1,\n",
       "          'set': 1,\n",
       "          'guideline': 1,\n",
       "          'intend': 1,\n",
       "          'help': 1,\n",
       "          'communicate': 1,\n",
       "          'appropriate': 1,\n",
       "          'action': 1,\n",
       "          'community': 1,\n",
       "          'follow': 1,\n",
       "          'potential': 1,\n",
       "          'situation': 1,\n",
       "          'release': 1,\n",
       "          'department': 1,\n",
       "          'health': 1,\n",
       "          'human': 1,\n",
       "          'service': 1,\n",
       "          'hhs': 1,\n",
       "          'february': 1,\n",
       "          '1': 1,\n",
       "          '2007': 1,\n",
       "          'design': 1,\n",
       "          'resemble': 1,\n",
       "          'saffir': 1,\n",
       "          'simpson': 1,\n",
       "          'hurricane': 1,\n",
       "          'scheme': 1,\n",
       "          'replace': 1,\n",
       "          'assessment': 1,\n",
       "          'framework': 1,\n",
       "          '2014': 1,\n",
       "          'use': 1,\n",
       "          'quadrant': 1,\n",
       "          'base': 1,\n",
       "          'transmissibility': 1,\n",
       "          'clinical': 1,\n",
       "          'linear': 1}),\n",
       " Counter({'plague': 4,\n",
       "          'cyprian': 2,\n",
       "          'pandemic': 2,\n",
       "          'afflict': 1,\n",
       "          'roman': 2,\n",
       "          'empire': 2,\n",
       "          'ad': 1,\n",
       "          '249': 1,\n",
       "          '262': 1,\n",
       "          'thought': 1,\n",
       "          'cause': 1,\n",
       "          'widespread': 1,\n",
       "          'manpower': 1,\n",
       "          'shortage': 1,\n",
       "          'food': 1,\n",
       "          'production': 1,\n",
       "          'army': 1,\n",
       "          'severely': 1,\n",
       "          'weaken': 1,\n",
       "          'crisis': 1,\n",
       "          'century': 1,\n",
       "          'modern': 1,\n",
       "          'commemorate': 1,\n",
       "          'st': 1,\n",
       "          'bishop': 1,\n",
       "          'carthage': 1,\n",
       "          'early': 1,\n",
       "          'christian': 1,\n",
       "          'writer': 1,\n",
       "          'witness': 1,\n",
       "          'describe': 1,\n",
       "          'agent': 1,\n",
       "          'highly': 1,\n",
       "          'speculative': 1,\n",
       "          'sparse': 1,\n",
       "          'source': 1,\n",
       "          'suspect': 1,\n",
       "          'include': 1,\n",
       "          'smallpox': 1,\n",
       "          'influenza': 1,\n",
       "          'viral': 1,\n",
       "          'hemorrhagic': 1,\n",
       "          'fever': 1,\n",
       "          'filovirus': 1,\n",
       "          'like': 1,\n",
       "          'ebola': 1,\n",
       "          'virus': 1}),\n",
       " Counter({'predict': 1,\n",
       "          'epidemiological': 1,\n",
       "          'research': 1,\n",
       "          'program': 2,\n",
       "          'fund': 1,\n",
       "          'united': 1,\n",
       "          'state': 1,\n",
       "          'agency': 1,\n",
       "          'international': 1,\n",
       "          'development': 1,\n",
       "          'usaid': 1,\n",
       "          'grant': 1,\n",
       "          '  ': 1,\n",
       "          'launch': 1,\n",
       "          '2009': 1,\n",
       "          'describe': 1,\n",
       "          'early': 1,\n",
       "          'warn': 1,\n",
       "          'pandemic': 1,\n",
       "          'system': 1}),\n",
       " Counter({'1929–1930': 2,\n",
       "          'psittacosis': 4,\n",
       "          'pandemic': 3,\n",
       "          'know': 2,\n",
       "          'outbreak': 5,\n",
       "          'great': 1,\n",
       "          'parrot': 11,\n",
       "          'fever': 3,\n",
       "          'series': 1,\n",
       "          'simultaneous': 1,\n",
       "          'accelerate': 1,\n",
       "          'breed': 1,\n",
       "          'transportation': 1,\n",
       "          'bird': 7,\n",
       "          'crowd': 1,\n",
       "          'container': 1,\n",
       "          'purpose': 1,\n",
       "          'trade': 3,\n",
       "          'initially': 1,\n",
       "          'see': 1,\n",
       "          'origin': 3,\n",
       "          'south': 2,\n",
       "          'america': 2,\n",
       "          'shortly': 1,\n",
       "          'find': 1,\n",
       "          'spread': 1,\n",
       "          'species': 1,\n",
       "          'country': 3,\n",
       "          'worldwide': 1,\n",
       "          'human': 2,\n",
       "          'mid': 2,\n",
       "          '1929': 3,\n",
       "          'early': 1,\n",
       "          '1930': 2,\n",
       "          'diagnose': 1,\n",
       "          'clinical': 1,\n",
       "          'feature': 1,\n",
       "          'link': 3,\n",
       "          'affect': 2,\n",
       "          '750': 1,\n",
       "          '800': 1,\n",
       "          'people': 1,\n",
       "          'globally': 1,\n",
       "          'mortality': 1,\n",
       "          '15': 1,\n",
       "          'mode': 1,\n",
       "          'transmission': 1,\n",
       "          'mouth': 1,\n",
       "          'beak': 1,\n",
       "          'contact': 1,\n",
       "          'inhale': 1,\n",
       "          'dry': 1,\n",
       "          'secretion': 1,\n",
       "          'dropping': 1,\n",
       "          'time': 1,\n",
       "          'cause': 1,\n",
       "          'chlamydia': 1,\n",
       "          'psittaci': 1,\n",
       "          'usually': 1,\n",
       "          'remain': 1,\n",
       "          'dormant': 1,\n",
       "          'activate': 1,\n",
       "          'stress': 1,\n",
       "          'capture': 1,\n",
       "          'confinement': 1,\n",
       "          'discover': 1,\n",
       "          '\\n ': 3,\n",
       "          'case': 5,\n",
       "          'report': 3,\n",
       "          'birmingham': 1,\n",
       "          'united': 2,\n",
       "          'kingdom': 1,\n",
       "          'buenos': 1,\n",
       "          'air': 1,\n",
       "          'argentina': 1,\n",
       "          'ongoing': 1,\n",
       "          'disease': 2,\n",
       "          'lead': 2,\n",
       "          'caution': 1,\n",
       "          'owner': 1,\n",
       "          'declare': 1,\n",
       "          'sick': 1,\n",
       "          'argentine': 3,\n",
       "          'city': 1,\n",
       "          'córdoba': 2,\n",
       "          'trace': 1,\n",
       "          'import': 1,\n",
       "          '5,000': 1,\n",
       "          'brazil': 1,\n",
       "          'stop': 1,\n",
       "          'number': 1,\n",
       "          'illegally': 1,\n",
       "          'sell': 2,\n",
       "          'visitor': 1,\n",
       "          'seaport': 1,\n",
       "          'consequence': 1,\n",
       "          'transmit': 1,\n",
       "          'november': 1,\n",
       "          'theatrical': 2,\n",
       "          'group': 2,\n",
       "          'local': 1,\n",
       "          'press': 2,\n",
       "          'january': 1,\n",
       "          'atypical': 1,\n",
       "          'pneumonia': 1,\n",
       "          'family': 1,\n",
       "          'death': 2,\n",
       "          'appear': 1,\n",
       "          'maryland': 1,\n",
       "          'state': 1,\n",
       "          'story': 1,\n",
       "          'headline': 1,\n",
       "          'american': 1,\n",
       "          'follow': 1,\n",
       "          'ban': 1,\n",
       "          'implement': 1,\n",
       "          'subsequently': 1,\n",
       "          'include': 2,\n",
       "          'germany': 1,\n",
       "          'france': 1,\n",
       "          'australia': 1,\n",
       "          'understand': 1,\n",
       "          'importation': 1,\n",
       "          'green': 1,\n",
       "          'amazon': 1,\n",
       "          'later': 1,\n",
       "          'principal': 1,\n",
       "          'source': 1,\n",
       "          'u.s': 2,\n",
       "          'domestic': 1,\n",
       "          'lovebird': 1,\n",
       "          'raise': 1,\n",
       "          'californian': 1,\n",
       "          'aviary': 1,\n",
       "          'mainly': 1,\n",
       "          'housewife': 1,\n",
       "          'widow': 1,\n",
       "          'impact': 1,\n",
       "          'hygienic': 1,\n",
       "          'laboratory': 1,\n",
       "          '16': 1,\n",
       "          'worker': 1,\n",
       "          'formation': 1,\n",
       "          'national': 1,\n",
       "          'institute': 1,\n",
       "          'health': 1}),\n",
       " Counter({'science': 7,\n",
       "          'diplomacy': 3,\n",
       "          'collaborative': 2,\n",
       "          'effort': 2,\n",
       "          'local': 1,\n",
       "          'global': 4,\n",
       "          'entity': 2,\n",
       "          'solve': 1,\n",
       "          'issue': 2,\n",
       "          'technology': 1,\n",
       "          'base': 1,\n",
       "          'collaboration': 1,\n",
       "          'take': 1,\n",
       "          'place': 1,\n",
       "          'advance': 2,\n",
       "          'facilitate': 1,\n",
       "          'diplomatic': 1,\n",
       "          'relation': 1,\n",
       "          'allow': 1,\n",
       "          'conflict': 1,\n",
       "          'nation': 1,\n",
       "          'come': 1,\n",
       "          'find': 1,\n",
       "          'solution': 1,\n",
       "          'organization': 1,\n",
       "          'researcher': 1,\n",
       "          'public': 1,\n",
       "          'health': 1,\n",
       "          'official': 2,\n",
       "          'country': 1,\n",
       "          'government': 1,\n",
       "          'clinician': 1,\n",
       "          'previously': 1,\n",
       "          'work': 1,\n",
       "          'create': 1,\n",
       "          'effective': 1,\n",
       "          'measure': 1,\n",
       "          'infection': 1,\n",
       "          'control': 1,\n",
       "          'subsequent': 1,\n",
       "          'treatment': 2,\n",
       "          'continue': 2,\n",
       "          'share': 1,\n",
       "          'resource': 1,\n",
       "          'research': 2,\n",
       "          'datum': 1,\n",
       "          'idea': 1,\n",
       "          'put': 1,\n",
       "          'effect': 1,\n",
       "          'law': 1,\n",
       "          'regulation': 1,\n",
       "          'scientific': 1,\n",
       "          'world': 1,\n",
       "          'vaccine': 1,\n",
       "          'possess': 1,\n",
       "          'disease': 2,\n",
       "          'consider': 1,\n",
       "          'deadly': 1,\n",
       "          'tuberculosis': 1,\n",
       "          'tetanus': 1,\n",
       "          'polio': 1,\n",
       "          'influenza': 1,\n",
       "          'etc': 1,\n",
       "          'historically': 1,\n",
       "          'prove': 1,\n",
       "          'successful': 1,\n",
       "          'sar': 1,\n",
       "          'ebola': 1,\n",
       "          'zika': 1,\n",
       "          'relevant': 1,\n",
       "          'covid-19': 1,\n",
       "          'pandemic': 1,\n",
       "          'today': 1}),\n",
       " Counter({'spanish': 4,\n",
       "          'flu': 6,\n",
       "          'know': 1,\n",
       "          '1918': 4,\n",
       "          'pandemic': 9,\n",
       "          'unusually': 1,\n",
       "          'deadly': 3,\n",
       "          'influenza': 6,\n",
       "          'cause': 2,\n",
       "          'h1n1': 2,\n",
       "          'virus': 3,\n",
       "          'last': 1,\n",
       "          'february': 1,\n",
       "          'april': 1,\n",
       "          '1920': 1,\n",
       "          'infect': 1,\n",
       "          '500': 1,\n",
       "          'million': 4,\n",
       "          'people': 1,\n",
       "          'world': 2,\n",
       "          'population': 1,\n",
       "          'time': 1,\n",
       "          'successive': 1,\n",
       "          'wave': 1,\n",
       "          'death': 2,\n",
       "          'toll': 1,\n",
       "          'typically': 2,\n",
       "          'estimate': 1,\n",
       "          '17': 1,\n",
       "          '50': 1,\n",
       "          'possibly': 1,\n",
       "          'high': 3,\n",
       "          '100': 1,\n",
       "          'make': 1,\n",
       "          'human': 1,\n",
       "          'history.the': 1,\n",
       "          'observation': 1,\n",
       "          'illness': 2,\n",
       "          'mortality': 3,\n",
       "          'document': 1,\n",
       "          'united': 2,\n",
       "          'state': 1,\n",
       "          'fort': 1,\n",
       "          'riley': 1,\n",
       "          'haskell': 1,\n",
       "          'county': 1,\n",
       "          'kansa': 1,\n",
       "          'new': 1,\n",
       "          'york': 1,\n",
       "          'city': 1,\n",
       "          'france': 1,\n",
       "          'brest': 1,\n",
       "          'germany': 1,\n",
       "          'kingdom': 1,\n",
       "          'maintain': 1,\n",
       "          'morale': 1,\n",
       "          'war': 2,\n",
       "          'censor': 1,\n",
       "          'minimize': 1,\n",
       "          'early': 1,\n",
       "          'report': 2,\n",
       "          'newspaper': 1,\n",
       "          'free': 1,\n",
       "          'epidemic': 1,\n",
       "          'effect': 1,\n",
       "          'neutral': 1,\n",
       "          'spain': 2,\n",
       "          'grave': 1,\n",
       "          'king': 1,\n",
       "          'alfonso': 1,\n",
       "          'xiii': 1,\n",
       "          'story': 1,\n",
       "          'create': 1,\n",
       "          'false': 1,\n",
       "          'impression': 1,\n",
       "          'especially': 1,\n",
       "          'hard': 1,\n",
       "          'hit': 1,\n",
       "          'give': 1,\n",
       "          'rise': 1,\n",
       "          'historical': 1,\n",
       "          'epidemiological': 1,\n",
       "          'datum': 1,\n",
       "          'inadequate': 1,\n",
       "          'identify': 1,\n",
       "          'certainty': 1,\n",
       "          'geographic': 1,\n",
       "          'origin': 1,\n",
       "          'vary': 1,\n",
       "          'view': 1,\n",
       "          'location': 1,\n",
       "          '\\n ': 1,\n",
       "          'outbreak': 1,\n",
       "          'disproportionately': 1,\n",
       "          'kill': 2,\n",
       "          'young': 3,\n",
       "          'old': 1,\n",
       "          'survival': 1,\n",
       "          'rate': 3,\n",
       "          'result': 1,\n",
       "          'higher': 1,\n",
       "          'expect': 1,\n",
       "          'adult': 2,\n",
       "          'scientist': 1,\n",
       "          'offer': 1,\n",
       "          'possible': 1,\n",
       "          'explanation': 1,\n",
       "          'analysis': 2,\n",
       "          'show': 1,\n",
       "          'particularly': 1,\n",
       "          'trigger': 1,\n",
       "          'cytokine': 1,\n",
       "          'storm': 1,\n",
       "          'ravage': 1,\n",
       "          'strong': 1,\n",
       "          'immune': 1,\n",
       "          'system': 1,\n",
       "          'contrast': 1,\n",
       "          '2007': 1,\n",
       "          'medical': 2,\n",
       "          'journal': 1,\n",
       "          'period': 1,\n",
       "          'find': 1,\n",
       "          'viral': 1,\n",
       "          'infection': 1,\n",
       "          'aggressive': 1,\n",
       "          'previous': 1,\n",
       "          'strain': 1,\n",
       "          'instead': 1,\n",
       "          'malnourishment': 1,\n",
       "          'overcrowded': 1,\n",
       "          'camp': 1,\n",
       "          'hospital': 1,\n",
       "          'poor': 1,\n",
       "          'hygiene': 1,\n",
       "          'exacerbate': 1,\n",
       "          'recent': 1,\n",
       "          'promote': 1,\n",
       "          'bacterial': 1,\n",
       "          'superinfection': 2,\n",
       "          'victim': 1,\n",
       "          'somewhat': 1,\n",
       "          'prolong': 1,\n",
       "          'bed.the': 1,\n",
       "          'second': 1,\n",
       "          '2009': 1,\n",
       "          'swine': 1}),\n",
       " Counter({'superspreader': 5,\n",
       "          'unusually': 1,\n",
       "          'contagious': 1,\n",
       "          'organism': 1,\n",
       "          'infect': 4,\n",
       "          'disease': 1,\n",
       "          'context': 1,\n",
       "          'human': 1,\n",
       "          'borne': 1,\n",
       "          'illness': 1,\n",
       "          'individual': 3,\n",
       "          'likely': 1,\n",
       "          'compare': 1,\n",
       "          'typical': 1,\n",
       "          'infected': 1,\n",
       "          'person': 1,\n",
       "          'particular': 1,\n",
       "          'concern': 1,\n",
       "          'epidemiology': 1,\n",
       "          '\\n ': 1,\n",
       "          'case': 1,\n",
       "          'superspreade': 1,\n",
       "          'conform': 1,\n",
       "          '80/20': 1,\n",
       "          'rule': 1,\n",
       "          'approximately': 1,\n",
       "          '20': 1,\n",
       "          'responsible': 1,\n",
       "          '80': 1,\n",
       "          'transmission': 2,\n",
       "          'superspreading': 1,\n",
       "          'say': 1,\n",
       "          'occur': 1,\n",
       "          'account': 1,\n",
       "          'high': 1,\n",
       "          'low': 1,\n",
       "          'percentage': 1,\n",
       "          '  ': 1,\n",
       "          'epidemic': 1,\n",
       "          'event': 1,\n",
       "          'ssev': 1,\n",
       "          'majority': 1,\n",
       "          'relatively': 1,\n",
       "          'secondary': 1,\n",
       "          'contacts.ssevs': 1,\n",
       "          'shape': 1,\n",
       "          'multiple': 1,\n",
       "          'factor': 1,\n",
       "          'include': 1,\n",
       "          'decline': 1,\n",
       "          'herd': 1,\n",
       "          'immunity': 1,\n",
       "          'nosocomial': 1,\n",
       "          'infection': 2,\n",
       "          'virulence': 1,\n",
       "          'viral': 1,\n",
       "          'load': 1,\n",
       "          'misdiagnosis': 1,\n",
       "          'airflow': 1,\n",
       "          'dynamic': 1,\n",
       "          'immune': 1,\n",
       "          'suppression': 1,\n",
       "          'co': 1,\n",
       "          'pathogen': 1}),\n",
       " Counter({'swine': 11,\n",
       "          'influenza': 11,\n",
       "          'infection': 2,\n",
       "          'cause': 2,\n",
       "          'type': 1,\n",
       "          'virus': 5,\n",
       "          'siv': 2,\n",
       "          'origin': 1,\n",
       "          's': 1,\n",
       "          'oiv': 1,\n",
       "          'strain': 3,\n",
       "          'family': 1,\n",
       "          'viruse': 1,\n",
       "          'endemic': 1,\n",
       "          'pig': 4,\n",
       "          '2009': 2,\n",
       "          'know': 2,\n",
       "          'include': 1,\n",
       "          'c': 1,\n",
       "          'subtype': 2,\n",
       "          '  ': 1,\n",
       "          'h1n1': 1,\n",
       "          'h1n2': 1,\n",
       "          'h2n1': 1,\n",
       "          'h3n1': 1,\n",
       "          'h3n2': 1,\n",
       "          'h2n3': 1,\n",
       "          '\\n ': 4,\n",
       "          'common': 2,\n",
       "          'population': 2,\n",
       "          'worldwide': 2,\n",
       "          'transmission': 4,\n",
       "          'human': 7,\n",
       "          'lead': 1,\n",
       "          'flu': 10,\n",
       "          'result': 1,\n",
       "          'production': 1,\n",
       "          'antibodie': 1,\n",
       "          'blood': 1,\n",
       "          'call': 1,\n",
       "          'zoonotic': 2,\n",
       "          'people': 2,\n",
       "          'regular': 1,\n",
       "          'exposure': 1,\n",
       "          'increase': 1,\n",
       "          'risk': 1,\n",
       "          'mid-20th': 1,\n",
       "          'century': 1,\n",
       "          'identification': 1,\n",
       "          'possible': 2,\n",
       "          'allow': 1,\n",
       "          'accurate': 1,\n",
       "          'diagnosis': 1,\n",
       "          '50': 1,\n",
       "          'confirm': 1,\n",
       "          'rarely': 1,\n",
       "          'pass': 1,\n",
       "          'symptom': 1,\n",
       "          'similar': 1,\n",
       "          'like': 1,\n",
       "          'illness': 2,\n",
       "          'general': 2,\n",
       "          'chill': 1,\n",
       "          'fever': 1,\n",
       "          'sore': 1,\n",
       "          'throat': 1,\n",
       "          'muscle': 1,\n",
       "          'pain': 1,\n",
       "          'severe': 1,\n",
       "          'headache': 1,\n",
       "          'cough': 1,\n",
       "          'weakness': 1,\n",
       "          'shortness': 1,\n",
       "          'breath': 1,\n",
       "          'discomfort': 1,\n",
       "          'estimate': 2,\n",
       "          'pandemic': 3,\n",
       "          '11–21': 1,\n",
       "          'global': 1,\n",
       "          '6.8': 1,\n",
       "          'billion': 2,\n",
       "          '700': 1,\n",
       "          'million': 1,\n",
       "          '1.4': 1,\n",
       "          'contract': 1,\n",
       "          'absolute': 1,\n",
       "          'term': 1,\n",
       "          'spanish': 1,\n",
       "          'actual': 1,\n",
       "          'fatality': 2,\n",
       "          'range': 2,\n",
       "          '12,000': 1,\n",
       "          '18,000': 1,\n",
       "          '2012': 1,\n",
       "          'study': 1,\n",
       "          'cdc': 1,\n",
       "          '284,000': 1,\n",
       "          '150,000': 1,\n",
       "          '575,000': 1,\n",
       "          'august': 1,\n",
       "          '2010': 1,\n",
       "          'world': 1,\n",
       "          'health': 1,\n",
       "          'organization': 1,\n",
       "          'declare': 1,\n",
       "          'officially': 1,\n",
       "          'over.subsequent': 1,\n",
       "          'case': 2,\n",
       "          'report': 1,\n",
       "          'india': 1,\n",
       "          '2015': 2,\n",
       "          '31,156': 1,\n",
       "          'positive': 1,\n",
       "          'test': 1,\n",
       "          '1,841': 1,\n",
       "          'death': 1,\n",
       "          'march': 1}),\n",
       " Counter({'target': 1,\n",
       "          'immunization': 3,\n",
       "          'strategy': 3,\n",
       "          'approach': 1,\n",
       "          'design': 1,\n",
       "          'increase': 1,\n",
       "          'level': 1,\n",
       "          'population': 1,\n",
       "          'decrease': 1,\n",
       "          'chance': 1,\n",
       "          'epidemic': 2,\n",
       "          'outbreak': 2,\n",
       "          'regard': 1,\n",
       "          'use': 1,\n",
       "          'healthcare': 1,\n",
       "          'practice': 1,\n",
       "          'administration': 1,\n",
       "          'vaccine': 1,\n",
       "          'prevent': 1,\n",
       "          'biological': 2,\n",
       "          'refer': 1,\n",
       "          'general': 1,\n",
       "          'scheme': 1,\n",
       "          'complex': 1,\n",
       "          'network': 1,\n",
       "          'social': 1,\n",
       "          'artificial': 1,\n",
       "          'nature': 1,\n",
       "          'identification': 1,\n",
       "          'risk': 1,\n",
       "          'group': 1,\n",
       "          'individual': 1,\n",
       "          'high': 1,\n",
       "          'odd': 1,\n",
       "          'spread': 1,\n",
       "          'disease': 1,\n",
       "          'play': 1,\n",
       "          'important': 1,\n",
       "          'role': 1}),\n",
       " Counter({'unified': 1,\n",
       "          'victim': 1,\n",
       "          'identification': 2,\n",
       "          'system': 2,\n",
       "          'uvis': 7,\n",
       "          'internet': 1,\n",
       "          'enable': 1,\n",
       "          'database': 1,\n",
       "          'develop': 1,\n",
       "          'office': 2,\n",
       "          'chief': 2,\n",
       "          'medical': 2,\n",
       "          'examiner': 2,\n",
       "          'city': 5,\n",
       "          'new': 7,\n",
       "          'york': 6,\n",
       "          'ocme': 5,\n",
       "          'aftermath': 1,\n",
       "          'september': 1,\n",
       "          '11': 1,\n",
       "          'attack': 1,\n",
       "          'crash': 1,\n",
       "          'american': 1,\n",
       "          'airline': 1,\n",
       "          'flight': 1,\n",
       "          '587': 1,\n",
       "          '  ': 4,\n",
       "          'intend': 1,\n",
       "          'handle': 1,\n",
       "          'critical': 1,\n",
       "          'fatality': 1,\n",
       "          'management': 1,\n",
       "          'function': 1,\n",
       "          'necessary': 1,\n",
       "          'major': 1,\n",
       "          'disaster': 2,\n",
       "          'uvi': 1,\n",
       "          'strong': 2,\n",
       "          'flexible': 2,\n",
       "          'role': 2,\n",
       "          'base': 2,\n",
       "          'application': 2,\n",
       "          'permission': 2,\n",
       "          'control': 2,\n",
       "          'dynamically.in': 1,\n",
       "          'event': 2,\n",
       "          'mass': 1,\n",
       "          'casualty': 1,\n",
       "          'initially': 1,\n",
       "          '311': 1,\n",
       "          'center': 1,\n",
       "          'operator': 1,\n",
       "          'police': 1,\n",
       "          'department': 1,\n",
       "          'gather': 1,\n",
       "          'key': 1,\n",
       "          'information': 1,\n",
       "          'facilitate': 2,\n",
       "          'compile': 1,\n",
       "          'accurate': 1,\n",
       "          'list': 1,\n",
       "          'miss': 1,\n",
       "          'person': 1,\n",
       "          'track': 1,\n",
       "          'decedent': 1,\n",
       "          'collect': 1,\n",
       "          'postmortem': 1,\n",
       "          'finding': 1,\n",
       "          'process': 1,\n",
       "          'contain': 1,\n",
       "          'pandemic': 1,\n",
       "          'flu': 1,\n",
       "          'module': 1,\n",
       "          'prepare': 1,\n",
       "          'eventuality': 1,\n",
       "          'dynamically': 1,\n",
       "          '\\n ': 1,\n",
       "          'build': 1,\n",
       "          'nihilent': 1,\n",
       "          'consulting': 1,\n",
       "          'solution': 1,\n",
       "          'integration': 1,\n",
       "          'company': 2,\n",
       "          'lead': 1,\n",
       "          'design': 1,\n",
       "          'think': 1,\n",
       "          'know': 1,\n",
       "          'icra': 1,\n",
       "          'sapphire).develope': 1,\n",
       "          'public': 1,\n",
       "          'fund': 1,\n",
       "          'available': 1,\n",
       "          'municipality': 1,\n",
       "          'county': 1,\n",
       "          'state': 2,\n",
       "          'governmental': 1,\n",
       "          'agency': 1,\n",
       "          'charge': 1,\n",
       "          'license': 1,\n",
       "          'currently': 1,\n",
       "          'use': 1,\n",
       "          'jersey': 1,\n",
       "          'ocsme': 1}),\n",
       " Counter({'viral': 14,\n",
       "          'load': 6,\n",
       "          'know': 1,\n",
       "          'burden': 2,\n",
       "          'titre': 2,\n",
       "          'titer': 1,\n",
       "          'numerical': 1,\n",
       "          'expression': 1,\n",
       "          'quantity': 2,\n",
       "          'virus': 5,\n",
       "          'give': 2,\n",
       "          'volume': 1,\n",
       "          'fluid': 3,\n",
       "          'sputum': 1,\n",
       "          'blood': 2,\n",
       "          'plasma': 2,\n",
       "          'bodily': 1,\n",
       "          'example': 2,\n",
       "          'noroviru': 2,\n",
       "          'determine': 1,\n",
       "          'run': 1,\n",
       "          'water': 1,\n",
       "          'garden': 1,\n",
       "          'produce': 2,\n",
       "          'prolong': 1,\n",
       "          'shed': 1,\n",
       "          'ability': 1,\n",
       "          'survive': 1,\n",
       "          'environment': 1,\n",
       "          'minuscule': 1,\n",
       "          'infectious': 2,\n",
       "          'dose': 1,\n",
       "          'require': 1,\n",
       "          'infection': 3,\n",
       "          'human': 1,\n",
       "          '100': 1,\n",
       "          'particles.viral': 1,\n",
       "          'express': 1,\n",
       "          'particle': 2,\n",
       "          'ml': 2,\n",
       "          'depend': 1,\n",
       "          'type': 1,\n",
       "          'assay': 1,\n",
       "          'high': 1,\n",
       "          'correlate': 1,\n",
       "          'severity': 1,\n",
       "          'active': 1,\n",
       "          'calculate': 1,\n",
       "          'estimate': 1,\n",
       "          'live': 1,\n",
       "          'involve': 1,\n",
       "          'rna': 1,\n",
       "          'copy': 1,\n",
       "          'millilitre': 1,\n",
       "          '\\n ': 1,\n",
       "          'track': 1,\n",
       "          'monitor': 2,\n",
       "          'therapy': 1,\n",
       "          'chronic': 1,\n",
       "          'immunocompromise': 1,\n",
       "          'patient': 1,\n",
       "          'recover': 1,\n",
       "          'bone': 1,\n",
       "          'marrow': 1,\n",
       "          'solid': 1,\n",
       "          'organ': 1,\n",
       "          'transplantation': 1,\n",
       "          'currently': 1,\n",
       "          'routine': 1,\n",
       "          'testing': 1,\n",
       "          'available': 1,\n",
       "          'hiv-1': 1,\n",
       "          'cytomegalovirus': 1,\n",
       "          'hepatitis': 2,\n",
       "          'b': 1,\n",
       "          'c': 1,\n",
       "          'hiv': 3,\n",
       "          'particular': 1,\n",
       "          'interest': 1,\n",
       "          'treatment': 1,\n",
       "          'people': 1,\n",
       "          'continually': 1,\n",
       "          'discuss': 1,\n",
       "          'context': 1,\n",
       "          'management': 1,\n",
       "          'aid': 1}),\n",
       " Counter({'virus': 19,\n",
       "          'submicroscopic': 1,\n",
       "          'infectious': 2,\n",
       "          'agent': 1,\n",
       "          'replicate': 1,\n",
       "          'inside': 2,\n",
       "          'living': 1,\n",
       "          'cell': 7,\n",
       "          'organism': 3,\n",
       "          'infect': 8,\n",
       "          'type': 3,\n",
       "          'life': 5,\n",
       "          'form': 4,\n",
       "          'animal': 3,\n",
       "          'plant': 5,\n",
       "          'microorganism': 1,\n",
       "          'include': 2,\n",
       "          'bacteria': 3,\n",
       "          'archaea': 1,\n",
       "          '\\n ': 3,\n",
       "          'dmitri': 1,\n",
       "          'ivanovsky': 1,\n",
       "          '1892': 1,\n",
       "          'article': 1,\n",
       "          'describe': 3,\n",
       "          'non': 1,\n",
       "          'bacterial': 1,\n",
       "          'pathogen': 1,\n",
       "          'tobacco': 2,\n",
       "          'discovery': 1,\n",
       "          'mosaic': 1,\n",
       "          'martinus': 1,\n",
       "          'beijerinck': 1,\n",
       "          '1898': 1,\n",
       "          '6,000': 1,\n",
       "          '  ': 1,\n",
       "          'specie': 3,\n",
       "          'detail': 1,\n",
       "          'million': 1,\n",
       "          'viruse': 5,\n",
       "          'environment': 1,\n",
       "          'find': 1,\n",
       "          'ecosystem': 1,\n",
       "          'earth': 1,\n",
       "          'numerous': 1,\n",
       "          'biological': 1,\n",
       "          'entity': 1,\n",
       "          'study': 1,\n",
       "          'know': 2,\n",
       "          'virology': 1,\n",
       "          'subspeciality': 1,\n",
       "          'microbiology': 1,\n",
       "          'infected': 2,\n",
       "          'host': 3,\n",
       "          'force': 1,\n",
       "          'rapidly': 1,\n",
       "          'produce': 3,\n",
       "          'thousand': 1,\n",
       "          'identical': 1,\n",
       "          'copy': 1,\n",
       "          'original': 1,\n",
       "          'process': 1,\n",
       "          'exist': 1,\n",
       "          'independent': 1,\n",
       "          'particle': 3,\n",
       "          'virion': 2,\n",
       "          'consist': 1,\n",
       "          'genetic': 4,\n",
       "          'material': 3,\n",
       "          'i.e.': 1,\n",
       "          'long': 1,\n",
       "          'molecule': 1,\n",
       "          'dna': 2,\n",
       "          'rna': 1,\n",
       "          'encode': 1,\n",
       "          'structure': 3,\n",
       "          'protein': 2,\n",
       "          'act': 1,\n",
       "          'ii': 1,\n",
       "          'coat': 1,\n",
       "          'capsid': 1,\n",
       "          'surround': 1,\n",
       "          'protect': 1,\n",
       "          'case': 1,\n",
       "          'iii': 1,\n",
       "          'outside': 1,\n",
       "          'envelope': 1,\n",
       "          'lipid': 1,\n",
       "          'shape': 1,\n",
       "          'range': 2,\n",
       "          'simple': 1,\n",
       "          'helical': 1,\n",
       "          'icosahedral': 1,\n",
       "          'complex': 1,\n",
       "          'small': 1,\n",
       "          'see': 1,\n",
       "          'optical': 1,\n",
       "          'microscope': 1,\n",
       "          'hundredth': 1,\n",
       "          'size': 1,\n",
       "          'origin': 1,\n",
       "          'evolutionary': 1,\n",
       "          'history': 1,\n",
       "          'unclear': 1,\n",
       "          'evolve': 3,\n",
       "          'plasmid': 1,\n",
       "          'piece': 1,\n",
       "          'evolution': 1,\n",
       "          'important': 1,\n",
       "          'mean': 2,\n",
       "          'horizontal': 1,\n",
       "          'gene': 1,\n",
       "          'transfer': 1,\n",
       "          'increase': 1,\n",
       "          'diversity': 1,\n",
       "          'way': 2,\n",
       "          'analogous': 1,\n",
       "          'sexual': 2,\n",
       "          'reproduction': 1,\n",
       "          'consider': 2,\n",
       "          'biologist': 1,\n",
       "          'carry': 2,\n",
       "          'reproduce': 1,\n",
       "          'natural': 1,\n",
       "          'selection': 1,\n",
       "          'lack': 1,\n",
       "          'key': 1,\n",
       "          'characteristic': 1,\n",
       "          'generally': 1,\n",
       "          'necessary': 1,\n",
       "          'criterion': 1,\n",
       "          'possess': 1,\n",
       "          'quality': 1,\n",
       "          'edge': 1,\n",
       "          'self-replicators.viruse': 1,\n",
       "          'spread': 2,\n",
       "          'transmission': 1,\n",
       "          'pathway': 1,\n",
       "          'disease': 1,\n",
       "          'bear': 1,\n",
       "          'vector': 1,\n",
       "          'example': 1,\n",
       "          'transmit': 3,\n",
       "          'insect': 2,\n",
       "          'feed': 1,\n",
       "          'sap': 1,\n",
       "          'aphid': 1,\n",
       "          'blood': 2,\n",
       "          'suck': 1,\n",
       "          'influenza': 1,\n",
       "          'cough': 1,\n",
       "          'sneeze': 1,\n",
       "          'noroviru': 2,\n",
       "          'rotavirus': 1,\n",
       "          'common': 1,\n",
       "          'cause': 2,\n",
       "          'viral': 3,\n",
       "          'gastroenteritis': 1,\n",
       "          'faecal': 1,\n",
       "          'oral': 1,\n",
       "          'route': 1,\n",
       "          'pass': 1,\n",
       "          'hand': 1,\n",
       "          'mouth': 1,\n",
       "          'contact': 2,\n",
       "          'food': 1,\n",
       "          'water': 1,\n",
       "          'dose': 1,\n",
       "          'require': 1,\n",
       "          'infection': 5,\n",
       "          'human': 1,\n",
       "          '100': 1,\n",
       "          'hiv': 1,\n",
       "          'exposure': 1,\n",
       "          'variety': 1,\n",
       "          'call': 1,\n",
       "          'narrow': 1,\n",
       "          'meaning': 1,\n",
       "          'capable': 2,\n",
       "          'broad': 1,\n",
       "          'many.viral': 1,\n",
       "          'provoke': 1,\n",
       "          'immune': 3,\n",
       "          'response': 3,\n",
       "          'usually': 1,\n",
       "          'eliminate': 1,\n",
       "          'vaccine': 1,\n",
       "          'confer': 1,\n",
       "          'artificially': 1,\n",
       "          'acquire': 1,\n",
       "          'immunity': 1,\n",
       "          'specific': 1,\n",
       "          'aid': 1,\n",
       "          'hpv': 1,\n",
       "          'hepatitis': 1,\n",
       "          'evade': 1,\n",
       "          'result': 1,\n",
       "          'chronic': 1,\n",
       "          'antiviral': 1,\n",
       "          'drug': 1,\n",
       "          'develop': 1})]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6cc79a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_docs_with_token  = {}\n",
    "for token in vocab:\n",
    "   # For each token in corpus vocabulary, count in how many documents it occurs\n",
    "    doc_count = 0\n",
    "    for document in data:\n",
    "        if token in document['tokenized_text']:\n",
    "            doc_count = doc_count + 1\n",
    "    number_docs_with_token[token] = doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "92f130a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 16, 2, 3, 4, 3, 2, 2, 2]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v for v in number_docs_with_token.values() if v > 1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "1b3333d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_docs_with_token['ebola']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e9d2b",
   "metadata": {},
   "source": [
    "## Calculate Tf-Idf vectors for every article in the dataset and add these vectors to the article dictionaries. \n",
    "\n",
    "You should end up the same list of dictionaries as before, but with a new key-value pair containing Tf-Idf vectors:\n",
    "\n",
    "- title: Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- text: Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- tokenized_text: Tokenized Wikipedia article text.\n",
    "\n",
    "\n",
    "- url: Link to the Wikipedia article.\n",
    "\n",
    "\n",
    "- tf_idfs: Tf_Idf vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6693b",
   "metadata": {},
   "source": [
    "$tf = \\frac{count(token\\:in\\:document)}{count(all\\tokens\\:in\\:document)}$\n",
    "\n",
    "\n",
    "$idf(token) = \\frac{number\\:of\\:documents}{number\\:of\\:documents\\:containing\\:the\\:token}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "10d2d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs_token_counter):\n",
    "    doc_length = len(doc)\n",
    "    tfidf_vec = []\n",
    "    for token in vocab:\n",
    "        # compute a term frequency (tf) per document\n",
    "        tf = doc[token] / len(data[i][\"tokenized_text\"])\n",
    "\n",
    "        # compute a log of inverse document frequency per document\n",
    "        idf = np.log(len(data)/number_docs_with_token[token])\n",
    "        \n",
    "        # Compute tfidf for the token and append to a list of TfIdfs for this document\n",
    "        tfidf_vec.append(tf*idf)\n",
    "    \n",
    "    # add tf_idf vector to the corresponding data dictionary\n",
    "    data[i]['tfidf_vec'] = tfidf_vec       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "d55486c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an updates summary with computed Tf-Idf vectors\n",
    "with open('data/summaries.json', 'w') as json_file:\n",
    "    json.dump(data, json_file)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "f0290229",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"highest pandemic casualties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "48413726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the workflow for article Tf-Idf calculation\n",
    "# to build a vectorizer function for search queries\n",
    "\n",
    "def vectorize(query, vocab = vocab): \n",
    "    query_vec = []\n",
    "    # Tokenize query\n",
    "    tokenized_query = tokenizer(query)\n",
    "    query_length = len(tokenized_query)\n",
    "    # Count unique tokens in query\n",
    "    for token in vocab:\n",
    "        # Build a TfIdf vector of the same shape as the document TfIdfs\n",
    "        tf = Counter(tokenized_query)[token]/query_length\n",
    "        idf = np.log(len(data)/number_docs_with_token[token])\n",
    "        query_vec.append(tf*idf)\n",
    "        \n",
    "    return query_vec        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7573b2f9",
   "metadata": {},
   "source": [
    "## Now we can try to search our list of dictionaries using this Tf-Idf field using existing tools for similarity. \n",
    "\n",
    "We suggest you use scikit-learn library and its cosine_similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "5c2b8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = np.array(vectorize(query)).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "69fd2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_2 = np.array(data[0]['tfidf_vec']).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "c587a582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0160686743357183"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec_1, vec_2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "88b870dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a search function\n",
    "def search_tfidf(query, docs):    \n",
    "    # vectorize query\n",
    "    vectorized_query = vectorize(query)\n",
    "    # Build a list of results using sklearn cosine_similarity function\n",
    "    rankings = []\n",
    "    for doc in docs:\n",
    "        # compute cosine similarity rank\n",
    "        rank = cosine_similarity(np.array(vectorized_query).reshape(1, -1), np.array(doc['tfidf_vec']).reshape(1, -1))[0][0]\n",
    "        if rank > 0:\n",
    "            # add this document to results along with its similarity rank\n",
    "            rankings.append({'title': doc['title'], 'rank': rank})\n",
    "     \n",
    "    # The results should be a list of dictionaries with at least the 'title' and 'rank' fields\n",
    "    return sorted(rankings, key=lambda item: item.get(\"rank\"), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "2301451f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Plague of Cyprian', 'rank': 0.11778345241757451},\n",
       " {'title': 'Science diplomacy and pandemics', 'rank': 0.07118947137494436}]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test how well this fuction works\n",
    "search_tfidf(\"ebola\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "b0137a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Plague of Cyprian was a pandemic that afflicted the Roman Empire about from AD 249 to 262. The plague is thought to have caused widespread manpower shortages for food production and the Roman army, severely weakening the empire during the Crisis of the Third Century. Its modern name commemorates St. Cyprian, bishop of Carthage, an early Christian writer who witnessed and described the plague. The agent of the plague is highly speculative because of sparse sourcing, but suspects have included smallpox, pandemic influenza and viral hemorrhagic fever (filoviruses) like the Ebola virus.\n"
     ]
    }
   ],
   "source": [
    "for s in data:\n",
    "    if s[\"title\"] == 'Plague of Cyprian':\n",
    "        print(s[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd4417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
