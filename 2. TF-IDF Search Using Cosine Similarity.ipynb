{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5dcf6d",
   "metadata": {},
   "source": [
    "# M1 Text Processing Using spaCy Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0b690",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Preprocess the dataset using spaCy library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65482bb8",
   "metadata": {},
   "source": [
    "## Load all relevant Python libraries and a spaCy language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e32ac537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1fd9b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "16d04b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = sp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b0e9e4",
   "metadata": {},
   "source": [
    "##  Open the provided JSON file. \n",
    "\n",
    "It contains a list of dictionaries with summaries from Wikipedia articles, where each dictionary has three key-value pairs. The keys title, text and url correspond to:\n",
    "\n",
    "- Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- Link to the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "0e9e4116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'text', 'url'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/data.json', 'r') as outfile:\n",
    "    summaries = json.load(outfile)\n",
    "print(summaries[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "9e105282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pandemic (from Greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. A widespread endemic disease with a stable number of infected people is not a pandemic. Widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\\nThroughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. The most fatal pandemic in recorded history was the Black Death (also known as The Plague), which killed an estimated 75–200 million people in the 14th century. The term was not used yet but was for later pandemics including the 1918 influenza pandemic (Spanish flu). Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "036cb9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39465758",
   "metadata": {},
   "source": [
    "## Create a Python function that takes in a text string, performs all operations described in the previous step, and outputs a list of tokens (lemmas).\n",
    "\n",
    "- Lowercases the text string.\n",
    "\n",
    "\n",
    "- Creates a spaCy document with the text lemmas and their attributes using a spaCy model of your choice.\n",
    "\n",
    "\n",
    "- Removes stop words, punctuation, and other unclassified lemmas.\n",
    "\n",
    "\n",
    "- Returns a list of tokens (lemmas) found in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "09f12ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> a DET det a\n",
      "<class 'spacy.tokens.token.Token'> pandemic ADJ nsubj pandemic\n",
      "<class 'spacy.tokens.token.Token'> ( PUNCT punct (\n",
      "<class 'spacy.tokens.token.Token'> from ADP prep from\n",
      "<class 'spacy.tokens.token.Token'> greek ADJ amod greek\n"
     ]
    }
   ],
   "source": [
    "# Lowercase data. Lowercase the text\n",
    "# Explore the attributes of each token returned SpaCy.\n",
    "text = summaries[0]['text']\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized[:5]:\n",
    "    print(type(token), token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0dba0f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a pandemic (from greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. a widespread endemic disease with a stable number of infected people is not a pandemic. widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n",
       "throughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. the most fatal pandemic in recorded history was the black death (also known as the plague), which killed an estimated 75–200 million people in the 14th century. the term was not used yet but was for later pandemics including the 1918 influenza pandemic (spanish flu). current pandemics include covid-19 (sars-cov-2) and hiv/aids."
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "86927622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return sp(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "88c7809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a pandemic (from greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. a widespread endemic disease with a stable number of infected people is not a pandemic. widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n",
       "throughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. the most fatal pandemic in recorded history was the black death (also known as the plague), which killed an estimated 75–200 million people in the 14th century. the term was not used yet but was for later pandemics including the 1918 influenza pandemic (spanish flu). current pandemics include covid-19 (sars-cov-2) and hiv/aids."
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower(summaries[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8c0efc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation\n",
    "def remove_redundant_tokens(text_tokenized):\n",
    "    return ' '.join([token.text for token in text_tokenized if not token.is_stop and not token.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4f5c399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize (tokenize) the texts\n",
    "def lemmatize(text):\n",
    "    return [token.lemma_ for token in sp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a0a0a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tokenizer function\n",
    "def tokenizer(document):\n",
    "    \"\"\"\n",
    "    This function accepts a text string and:\n",
    "    1. Lowercases it\n",
    "    2. Removes redundant tokens\n",
    "    3. Performs token lemmatization\n",
    "    \"\"\" \n",
    "    text_tokenized = lower(document)\n",
    "    clean_text =  remove_redundant_tokens(text_tokenized)\n",
    "    token_lemmatized = lemmatize(clean_text)\n",
    "    return token_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334f8bc",
   "metadata": {},
   "source": [
    "## Use this function to preprocess all text documents in the dataset (text field only), and add the resulting lists to the dictionaries from step 1. \n",
    "\n",
    "You should end up with a list of dictionaries, each of which now has four key-value pairs:\n",
    "\n",
    "- title: Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- text: Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- tokenized_text: Tokenized Wikipedia article text.\n",
    "\n",
    "\n",
    "- url: Link to the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4f96b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all the documents using the tokenizer function\n",
    "for doc in summaries:\n",
    "    doc['tokenized_text'] = tokenizer(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "77c3aed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pandemic',\n",
       " 'greek',\n",
       " 'πᾶν',\n",
       " 'pan',\n",
       " 'δῆμος',\n",
       " 'demos',\n",
       " 'people',\n",
       " 'epidemic',\n",
       " 'infectious',\n",
       " 'disease']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]['tokenized_text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "69d4d918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries[0]['tokenized_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9785ef",
   "metadata": {},
   "source": [
    "## Save the new list of dictionaries in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6c97cf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='data/summaries.json' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenized texts to file:\n",
    "with open('data/summaries.json', 'w') as outfile:\n",
    "    json.dump(summaries, outfile)\n",
    "outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c22338",
   "metadata": {},
   "source": [
    "# M2 TF-IDF Search Using Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab2c60",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Implement a basic Tf-Idf search.\n",
    "\n",
    "- In your search for an optimal document retrieval method in the CDC’s huge knowledge base, you decide to try the term frequency search first because of its simplicity. It is a well-developed technique and is a great place to start!\n",
    "\n",
    "\n",
    "- In Milestone 1, you prepared the documents for Tf-Idf-based search. You also computed the Tf-Idf vectors for every document in the CDC’s knowledge base. The standard approach to finding the most relevant documents to your query is to compute similarities between the Tf-Idf vectors of the documents and the query. It works, but you realize that it can be very inefficient for very large document sets since you need to compute the similarities between the query and every one of the documents in your database. What would be a better solution? Let us move on to the last milestone of the project to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60074fe",
   "metadata": {},
   "source": [
    "## Load all relevant Python libraries and a spaCy language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9be9df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880bcc3",
   "metadata": {},
   "source": [
    "## Access the tokenized text in your new dataset from the previous milestone. \n",
    "\n",
    "Each document dictionary should now include a new key-value pair with the lemmatized text of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "2e071283",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/summaries.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f78980",
   "metadata": {},
   "source": [
    "## Create a corpus vocabulary. It should simply be a list of unique tokens in the provided set of documents. \n",
    "\n",
    "Count how many times each unique token appears in the corpus, you will need these counts for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "9e380e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n",
      "(3617,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1506"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all tokenized texts into a single list\n",
    "tokenized_texts = [i[\"tokenized_text\"] for i in data]\n",
    "print(np.array(tokenized_texts).shape)\n",
    "\n",
    "# flatten the list of lists (use itertools.chain)\n",
    "flattened_tokenized_texts = list(itertools.chain(*tokenized_texts))\n",
    "print(np.array(flattened_tokenized_texts).shape)\n",
    "\n",
    "# remove duplicates\n",
    "vocab = list(set(flattened_tokenized_texts))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "e50bf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary as a json file\n",
    "with open('data/vocab.json', 'w') as outfile:\n",
    "    json.dump(vocab, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "0926d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times each token occurs in a document\n",
    "# you will need it for TfIdf calculations\n",
    "docs_token_counter = []\n",
    "for doc in data:\n",
    "    # For each document, count how many of each token they have\n",
    "    # Counter function from collections is very handy\n",
    "    docs_token_counter.append([y for x, y in Counter(doc['tokenized_text']).items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "50a18d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6cc79a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_docs_with_token  = {}\n",
    "for token in vocab:\n",
    "   # For each token in corpus vocabulary, count in how many documents it occurs\n",
    "    doc_count = 0\n",
    "    for document in data:\n",
    "        if token in document['tokenized_text']:\n",
    "            doc_count = doc_count + 1\n",
    "    number_docs_with_token[token] = doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "92f130a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 16, 2, 3, 4, 3, 2, 2, 2]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v for v in number_docs_with_token.values() if v > 1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "1b3333d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_docs_with_token['ebola']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e9d2b",
   "metadata": {},
   "source": [
    "## Calculate Tf-Idf vectors for every article in the dataset and add these vectors to the article dictionaries. \n",
    "\n",
    "You should end up the same list of dictionaries as before, but with a new key-value pair containing Tf-Idf vectors:\n",
    "\n",
    "- title: Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- text: Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- tokenized_text: Tokenized Wikipedia article text.\n",
    "\n",
    "\n",
    "- url: Link to the Wikipedia article.\n",
    "\n",
    "\n",
    "- tf_idfs: Tf_Idf vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6693b",
   "metadata": {},
   "source": [
    "$tf = \\frac{count(token\\:in\\:document)}{count(all\\tokens\\:in\\:document)}$\n",
    "\n",
    "\n",
    "$idf(token) = \\frac{number\\:of\\:documents}{number\\:of\\:documents\\:containing\\:the\\:token}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "10d2d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs_token_counter):\n",
    "    doc_length = len(doc)\n",
    "    tfidf_vec = []\n",
    "    for token in vocab:\n",
    "        # compute a term frequency (tf) per document\n",
    "        tf = doc[token] / len(data[i][\"tokenized_text\"])\n",
    "\n",
    "        # compute a log of inverse document frequency per document\n",
    "        idf = np.log(len(data)/number_docs_with_token[token])\n",
    "        \n",
    "        # Compute tfidf for the token and append to a list of TfIdfs for this document\n",
    "        tfidf_vec.append(tf*idf)\n",
    "    \n",
    "    # add tf_idf vector to the corresponding data dictionary\n",
    "    data[i]['tfidf_vec'] = tfidf_vec       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "d55486c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an updates summary with computed Tf-Idf vectors\n",
    "with open('data/summaries.json', 'w') as json_file:\n",
    "    json.dump(data, json_file)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "f0290229",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"highest pandemic casualties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "48413726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the workflow for article Tf-Idf calculation\n",
    "# to build a vectorizer function for search queries\n",
    "\n",
    "def vectorize(query, vocab = vocab): \n",
    "    query_vec = []\n",
    "    # Tokenize query\n",
    "    tokenized_query = tokenizer(query)\n",
    "    query_length = len(tokenized_query)\n",
    "    # Count unique tokens in query\n",
    "    for token in vocab:\n",
    "        # Build a TfIdf vector of the same shape as the document TfIdfs\n",
    "        tf = Counter(tokenized_query)[token]/query_length\n",
    "        idf = np.log(len(data)/number_docs_with_token[token])\n",
    "        query_vec.append(tf*idf)\n",
    "        \n",
    "    return query_vec        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7573b2f9",
   "metadata": {},
   "source": [
    "## Now we can try to search our list of dictionaries using this Tf-Idf field using existing tools for similarity. \n",
    "\n",
    "We suggest you use scikit-learn library and its cosine_similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "5c2b8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = np.array(vectorize(query)).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "69fd2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_2 = np.array(data[0]['tfidf_vec']).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "c587a582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0160686743357183"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec_1, vec_2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "88b870dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a search function\n",
    "def search_tfidf(query, docs):    \n",
    "    # vectorize query\n",
    "    vectorized_query = vectorize(query)\n",
    "    # Build a list of results using sklearn cosine_similarity function\n",
    "    rankings = []\n",
    "    for doc in docs:\n",
    "        # compute cosine similarity rank\n",
    "        rank = cosine_similarity(np.array(vectorized_query).reshape(1, -1), np.array(doc['tfidf_vec']).reshape(1, -1))[0][0]\n",
    "        if rank > 0:\n",
    "            # add this document to results along with its similarity rank\n",
    "            rankings.append({'title': doc['title'], 'rank': rank})\n",
    "     \n",
    "    # The results should be a list of dictionaries with at least the 'title' and 'rank' fields\n",
    "    return sorted(rankings, key=lambda item: item.get(\"rank\"), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "2301451f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Plague of Cyprian', 'rank': 0.11778345241757451},\n",
       " {'title': 'Science diplomacy and pandemics', 'rank': 0.07118947137494436}]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test how well this fuction works\n",
    "search_tfidf(\"ebola\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "b0137a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Plague of Cyprian was a pandemic that afflicted the Roman Empire about from AD 249 to 262. The plague is thought to have caused widespread manpower shortages for food production and the Roman army, severely weakening the empire during the Crisis of the Third Century. Its modern name commemorates St. Cyprian, bishop of Carthage, an early Christian writer who witnessed and described the plague. The agent of the plague is highly speculative because of sparse sourcing, but suspects have included smallpox, pandemic influenza and viral hemorrhagic fever (filoviruses) like the Ebola virus.\n"
     ]
    }
   ],
   "source": [
    "for s in data:\n",
    "    if s[\"title\"] == 'Plague of Cyprian':\n",
    "        print(s[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd4417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
