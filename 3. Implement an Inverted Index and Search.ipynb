{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081a2d7d",
   "metadata": {},
   "source": [
    "# M1 Text Processing Using spaCy Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ebc71",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Preprocess the dataset using spaCy library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e12b04",
   "metadata": {},
   "source": [
    "## Load all relevant Python libraries and a spaCy language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "c2c678e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e4a3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "9c796882",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = sp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f3c63",
   "metadata": {},
   "source": [
    "##  Open the provided JSON file. \n",
    "\n",
    "It contains a list of dictionaries with summaries from Wikipedia articles, where each dictionary has three key-value pairs. The keys title, text and url correspond to:\n",
    "\n",
    "- Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- Link to the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "744feae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'text', 'url'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/data.json', 'r') as outfile:\n",
    "    summaries = json.load(outfile)\n",
    "print(summaries[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1767e889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pandemic (from Greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. A widespread endemic disease with a stable number of infected people is not a pandemic. Widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\\nThroughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. The most fatal pandemic in recorded history was the Black Death (also known as The Plague), which killed an estimated 75–200 million people in the 14th century. The term was not used yet but was for later pandemics including the 1918 influenza pandemic (Spanish flu). Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "eec66f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3e5b5",
   "metadata": {},
   "source": [
    "## Create a Python function that takes in a text string, performs all operations described in the previous step, and outputs a list of tokens (lemmas).\n",
    "\n",
    "- Lowercases the text string.\n",
    "\n",
    "\n",
    "- Creates a spaCy document with the text lemmas and their attributes using a spaCy model of your choice.\n",
    "\n",
    "\n",
    "- Removes stop words, punctuation, and other unclassified lemmas.\n",
    "\n",
    "\n",
    "- Returns a list of tokens (lemmas) found in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e44d5910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> a DET det a\n",
      "<class 'spacy.tokens.token.Token'> pandemic ADJ nsubj pandemic\n",
      "<class 'spacy.tokens.token.Token'> ( PUNCT punct (\n",
      "<class 'spacy.tokens.token.Token'> from ADP prep from\n",
      "<class 'spacy.tokens.token.Token'> greek ADJ amod greek\n"
     ]
    }
   ],
   "source": [
    "# Lowercase data. Lowercase the text\n",
    "# Explore the attributes of each token returned SpaCy.\n",
    "text = summaries[0]['text']\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized[:5]:\n",
    "    print(type(token), token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "576edf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a pandemic (from greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. a widespread endemic disease with a stable number of infected people is not a pandemic. widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n",
       "throughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. the most fatal pandemic in recorded history was the black death (also known as the plague), which killed an estimated 75–200 million people in the 14th century. the term was not used yet but was for later pandemics including the 1918 influenza pandemic (spanish flu). current pandemics include covid-19 (sars-cov-2) and hiv/aids."
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0f59a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return sp(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1901656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a pandemic (from greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. a widespread endemic disease with a stable number of infected people is not a pandemic. widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n",
       "throughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. the most fatal pandemic in recorded history was the black death (also known as the plague), which killed an estimated 75–200 million people in the 14th century. the term was not used yet but was for later pandemics including the 1918 influenza pandemic (spanish flu). current pandemics include covid-19 (sars-cov-2) and hiv/aids."
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower(summaries[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "902e209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation\n",
    "def remove_redundant_tokens(text_tokenized):\n",
    "    return ' '.join([token.text for token in text_tokenized if not token.is_stop and not token.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6292b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize (tokenize) the texts\n",
    "def lemmatize(text):\n",
    "    return [token.lemma_ for token in sp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b9940e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tokenizer function\n",
    "def tokenizer(document):\n",
    "    \"\"\"\n",
    "    This function accepts a text string and:\n",
    "    1. Lowercases it\n",
    "    2. Removes redundant tokens\n",
    "    3. Performs token lemmatization\n",
    "    \"\"\" \n",
    "    text_tokenized = lower(document)\n",
    "    clean_text =  remove_redundant_tokens(text_tokenized)\n",
    "    token_lemmatized = lemmatize(clean_text)\n",
    "    return token_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dfc6fc",
   "metadata": {},
   "source": [
    "## Use this function to preprocess all text documents in the dataset (text field only), and add the resulting lists to the dictionaries from step 1. \n",
    "\n",
    "You should end up with a list of dictionaries, each of which now has four key-value pairs:\n",
    "\n",
    "- title: Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- text: Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- tokenized_text: Tokenized Wikipedia article text.\n",
    "\n",
    "\n",
    "- url: Link to the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a67b7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all the documents using the tokenizer function\n",
    "for doc in summaries:\n",
    "    doc['tokenized_text'] = tokenizer(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f2678a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pandemic',\n",
       " 'greek',\n",
       " 'πᾶν',\n",
       " 'pan',\n",
       " 'δῆμος',\n",
       " 'demos',\n",
       " 'people',\n",
       " 'epidemic',\n",
       " 'infectious',\n",
       " 'disease']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]['tokenized_text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ca516d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries[0]['tokenized_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb08dc5",
   "metadata": {},
   "source": [
    "## Save the new list of dictionaries in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "67f8abce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='data/summaries.json' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenized texts to file:\n",
    "with open('data/summaries.json', 'w') as outfile:\n",
    "    json.dump(summaries, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e7098",
   "metadata": {},
   "source": [
    "# M2 TF-IDF Search Using Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5f66d",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Implement a basic Tf-Idf search.\n",
    "\n",
    "- In your search for an optimal document retrieval method in the CDC’s huge knowledge base, you decide to try the term frequency search first because of its simplicity. It is a well-developed technique and is a great place to start!\n",
    "\n",
    "\n",
    "- In Milestone 1, you prepared the documents for Tf-Idf-based search. You also computed the Tf-Idf vectors for every document in the CDC’s knowledge base. The standard approach to finding the most relevant documents to your query is to compute similarities between the Tf-Idf vectors of the documents and the query. It works, but you realize that it can be very inefficient for very large document sets since you need to compute the similarities between the query and every one of the documents in your database. What would be a better solution? Let us move on to the last milestone of the project to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047adf2f",
   "metadata": {},
   "source": [
    "## Load all relevant Python libraries and a spaCy language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7a5170cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ef300",
   "metadata": {},
   "source": [
    "## Access the tokenized text in your new dataset from the previous milestone. \n",
    "\n",
    "Each document dictionary should now include a new key-value pair with the lemmatized text of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "ae8f8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/summaries.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffaea5",
   "metadata": {},
   "source": [
    "## Create a corpus vocabulary. It should simply be a list of unique tokens in the provided set of documents. \n",
    "\n",
    "Count how many times each unique token appears in the corpus, you will need these counts for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "791dafbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n",
      "(3617,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1506"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all tokenized texts into a single list\n",
    "tokenized_texts = [i[\"tokenized_text\"] for i in data]\n",
    "print(np.array(tokenized_texts).shape)\n",
    "\n",
    "# flatten the list of lists (use itertools.chain)\n",
    "flattened_tokenized_texts = list(itertools.chain(*tokenized_texts))\n",
    "print(np.array(flattened_tokenized_texts).shape)\n",
    "\n",
    "# remove duplicates\n",
    "vocab = list(set(flattened_tokenized_texts))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "bbf89c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary as a json file\n",
    "with open('data/vocab.json', 'w') as outfile:\n",
    "    json.dump(vocab, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "427e513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times each token occurs in a document\n",
    "# you will need it for TfIdf calculations\n",
    "docs_token_counter = []\n",
    "for doc in data:\n",
    "    # For each document, count how many of each token they have\n",
    "    # Counter function from collections is very handy\n",
    "    docs_token_counter.append(Counter(doc['tokenized_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "b74f938d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "14034ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_docs_with_token  = {}\n",
    "for token in vocab:\n",
    "   # For each token in corpus vocabulary, count in how many documents it occurs\n",
    "    doc_count = 0\n",
    "    for document in data:\n",
    "        if token in document['tokenized_text']:\n",
    "            doc_count = doc_count + 1\n",
    "    number_docs_with_token[token] = doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "1a31d883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 16, 2, 3, 4, 3, 2, 2, 2]"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v for v in number_docs_with_token.values() if v > 1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "746644fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_docs_with_token['ebola']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043af6e2",
   "metadata": {},
   "source": [
    "## Calculate Tf-Idf vectors for every article in the dataset and add these vectors to the article dictionaries. \n",
    "\n",
    "You should end up the same list of dictionaries as before, but with a new key-value pair containing Tf-Idf vectors:\n",
    "\n",
    "- title: Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- text: Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- tokenized_text: Tokenized Wikipedia article text.\n",
    "\n",
    "\n",
    "- url: Link to the Wikipedia article.\n",
    "\n",
    "\n",
    "- tf_idfs: Tf_Idf vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e523fac",
   "metadata": {},
   "source": [
    "$tf = \\frac{count(token\\:in\\:document)}{count(all\\tokens\\:in\\:document)}$\n",
    "\n",
    "\n",
    "$idf(token) = \\frac{number\\:of\\:documents}{number\\:of\\:documents\\:containing\\:the\\:token}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "ea54178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'disease'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "09da4dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_token_counter[0][token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "9adc5f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[i][\"tokenized_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "b9e14044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045454545454545456"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = docs_token_counter[0][token]/len(data[0][\"tokenized_text\"])\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "f543cbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_docs_with_token[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "216abdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4855078157817008"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.log(len(data)/number_docs_with_token['disease'])\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "7e5334ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token_counter in enumerate(docs_token_counter):\n",
    "    tfidf_vec = []\n",
    "    for token in vocab:\n",
    "        # compute a term frequency (tf) per document\n",
    "        tf = token_counter[token] / len(data[i][\"tokenized_text\"])\n",
    "\n",
    "        # compute a log of inverse document frequency per document\n",
    "        idf = np.log(len(data)/number_docs_with_token[token])\n",
    "        \n",
    "        # Compute tfidf for the token and append to a list of tf_idfs for this document\n",
    "        tfidf_vec.append(tf*idf)\n",
    "    \n",
    "    # add tf_idf vector to the corresponding data dictionary\n",
    "    data[i]['tfidf_vec'] = tfidf_vec       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "b076fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an updates summary with computed Tf-Idf vectors\n",
    "with open('data/summaries.json', 'w') as json_file:\n",
    "    json.dump(data, json_file)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "5fd041dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"highest pandemic casualties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "1e970256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the workflow for article Tf-Idf calculation\n",
    "# to build a vectorizer function for search queries\n",
    "\n",
    "def vectorize(query, vocab = vocab): \n",
    "    query_vec = []\n",
    "    # Tokenize query\n",
    "    tokenized_query = tokenizer(query)\n",
    "    query_length = len(tokenized_query)\n",
    "    # Count unique tokens in query\n",
    "    for token in vocab:\n",
    "        # Build a TfIdf vector of the same shape as the document TfIdfs\n",
    "        tf = Counter(tokenized_query)[token]/query_length\n",
    "        idf = np.log(len(data)/number_docs_with_token[token])\n",
    "        query_vec.append(tf*idf)\n",
    "        \n",
    "    return query_vec        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d3981",
   "metadata": {},
   "source": [
    "## Now we can try to search our list of dictionaries using this Tf-Idf field using existing tools for similarity. \n",
    "\n",
    "We suggest you use scikit-learn library and its cosine_similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "14838bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = np.array(vectorize(query)).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "e29e50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_2 = np.array(data[0]['tfidf_vec']).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "17f2a1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0160686743357183"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec_1, vec_2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "0ed10120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a search function\n",
    "def search_tfidf(query, docs):\n",
    "    rankings = []\n",
    "    # vectorize query\n",
    "    try:\n",
    "        vectorized_query = vectorize(query)\n",
    "    except:\n",
    "        print(query)\n",
    "        return rankings\n",
    "    # Build a list of results using sklearn cosine_similarity function\n",
    "    for doc in docs:\n",
    "        # compute cosine similarity rank\n",
    "        rank = cosine_similarity(np.array(vectorized_query).reshape(1, -1), np.array(doc['tfidf_vec']).reshape(1, -1))[0][0]\n",
    "        if rank > 0:\n",
    "            # add this document to results along with its similarity rank\n",
    "            rankings.append({'title': doc['title'], 'rank': rank})\n",
    "     \n",
    "    # The results should be a list of dictionaries with at least the 'title' and 'rank' fields\n",
    "    return sorted(rankings, key=lambda item: item.get(\"rank\"), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "5138b22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Plague of Cyprian', 'rank': 0.11778345241757451},\n",
       " {'title': 'Science diplomacy and pandemics', 'rank': 0.07118947137494436}]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test how well this fuction works\n",
    "search_tfidf(\"ebola\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "feb7be29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Plague of Cyprian was a pandemic that afflicted the Roman Empire about from AD 249 to 262. The plague is thought to have caused widespread manpower shortages for food production and the Roman army, severely weakening the empire during the Crisis of the Third Century. Its modern name commemorates St. Cyprian, bishop of Carthage, an early Christian writer who witnessed and described the plague. The agent of the plague is highly speculative because of sparse sourcing, but suspects have included smallpox, pandemic influenza and viral hemorrhagic fever (filoviruses) like the Ebola virus.\n"
     ]
    }
   ],
   "source": [
    "for s in data:\n",
    "    if s[\"title\"] == 'Plague of Cyprian':\n",
    "        print(s[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477d108",
   "metadata": {},
   "source": [
    "# M3 Implement an Inverted Index and Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29369d52",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Implement an inverted index and search.\n",
    "\n",
    "- After testing out a simple Tf-Idf search, you realize that you can successfully search your repository of documents. However, the CDC’s library is enormous, and looping over every document to compute cosine similarities for every one of them does not seem like the best way to search. There must be a better way to do it!\n",
    "\n",
    "\n",
    "- Inverted index is the most commonly used data structure in document retrieval systems because it enables very fast full-text search. Instead of looking up query tokens in every document in the database, we can quickly retrieve the documents that are already known to contain the tokens by their keys. Our inclusion of Tf-Idf helps to take into account the number of times the token occurred in the document or its relevance.\n",
    "\n",
    "\n",
    "- The downside of using an inverted index is the increased processing cost. We have to tokenize every document and compute all the Tf-Idf values to be able to search them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe567a",
   "metadata": {},
   "source": [
    "## Create a new Jupyter Notebook and load dependencies required to complete this milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "79a34f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66544cd2",
   "metadata": {},
   "source": [
    "## Load the two JSON files you created and saved in Milestone 2:\n",
    "\n",
    "- The vocabulary file, containing all tokens in our corpus.\n",
    "\n",
    "\n",
    "- The file with the documents. Each document dictionary now should contain the following fields: title, text, URL, tokenized_text, tf_idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "d413a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/summaries.json', 'r') as f:\n",
    "    summaries = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "5215c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31200102",
   "metadata": {},
   "source": [
    "## Build an inverted index. \n",
    "\n",
    "- Use the previously calculated Tf_Idf values to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "7b343526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['COVID-19 pandemic', 0.0015344736613540093]]"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index = {}\n",
    "\n",
    "# Create a lookup dictionary for each word in vocabulary\n",
    "for i, word in enumerate(vocab):\n",
    "    inverted_index[word] = []  \n",
    "    # for each word in corpus vocabulary list all articles\n",
    "    # it occurs in and this word's TfIdf score for this article\n",
    "    for doc in summaries:\n",
    "        if word in doc['tokenized_text']:\n",
    "            inverted_index[word].append([doc['title'], np.array(doc['tfidf_vec']).mean(axis=0)])\n",
    "# Now you have a lookup table of all articles that have a particular keyword\n",
    "# lets request a list of articles with the word \"coronavirus\" in them\n",
    "inverted_index[\"coronavirus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "c9d00f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The disease was first identified in December 2019 in Wuhan, China. The outbreak was declared a Public Health Emergency of International Concern in January 2020, and a pandemic in March 2020. As of 17 October 2020, more than 39.5 million cases have been confirmed, with more than 1.1 million deaths attributed to COVID-19.\n",
      "\n",
      "Common symptoms include fever, cough, fatigue, breathing difficulties, and loss of smell. Complications may include pneumonia and acute respiratory distress syndrome. The incubation period is typically around five days but may range from one to 14 days. There are several vaccine candidates in development, although none have proven their safety and efficacy. There is no known specific antiviral medication, so primary treatment is currently symptomatic.\n",
      "Recommended preventive measures include hand washing, covering mouth or wearing face mask when sneezing or coughing, social distancing, disinfecting surfaces, ventilation and air-filtering, and monitoring and self-isolation if exposed or symptomatic. Travel restrictions, lockdowns, workplace hazard controls, and facility closures have been implemented. Many places have also worked to increase testing capacity and trace contacts of the infected. These have caused social and economic disruption, including the largest global recession since the Great Depression. Extreme poverty and global famines are affecting hundreds of millions, inflamed by supply shortages. Many events, the environment and education systems have also been affected. Misinformation about the virus has circulated globally. There have been many incidents of xenophobia and racism against Chinese people and against those perceived as being Chinese or as being from areas with high infection rates.\n"
     ]
    }
   ],
   "source": [
    "# Check if \"coronavirus\" is indeed in the article \n",
    "for s in summaries:\n",
    "    if s[\"title\"] == 'COVID-19 pandemic':\n",
    "        print(s[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d92a24",
   "metadata": {},
   "source": [
    "## Now that we have the index, we need a search function. \n",
    "\n",
    "Build a search function that accepts query texts, searches the inverted index, and returns sorted search results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02525f4",
   "metadata": {},
   "source": [
    "{\"query\": \"black death\", \"relevant_article_titles\": [[\"Pandemic\", 0.047336756359133265], [\"Cholera\", 0.014518164813892178], [\"Antonine Plague\", 0.013233865618817103], [\"Epidemiology of HIV/AIDS\", 0.011947239794765438], [\"Bills of mortality\", 0.008868054280650635], [\"Spanish flu\", 0.008602012652231115], [\"1929\\u20131930 psittacosis pandemic\", 0.008231591054766618], [\"Pandemic Severity Assessment Framework\", 0.008039264160963658], [\"HIV/AIDS\", 0.006826994168437393], [\"COVID-19 pandemic\", 0.005060007442488892], [\"Swine influenza\", 0.004675006876212563]]}, {\"query\": \"zoonotic diseases\", \"relevant_article_titles\": [[\"Swine influenza\", 0.035414092804581326], [\"Disease X\", 0.029604135108640295], [\"Pandemic\", 0.022322198426744867], [\"Pandemic prevention\", 0.013871651879477165], [\"HIV/AIDS\", 0.013486328216158356], [\"Targeted immunization strategies\", 0.0105545177343848], [\"Science diplomacy and pandemics\", 0.01032995352727023], [\"HIV/AIDS in Yunnan\", 0.008593058686401785], [\"Cholera\", 0.008194224738931659], [\"Antonine Plague\", 0.007469351012026167], [\"Superspreader\", 0.0066507919970096], [\"COVID-19 pandemic\", 0.005711856656255304], [\"Basic reproduction number\", 0.0056020132590196255], [\"1929\\u20131930 psittacosis pandemic\", 0.004646007806523453], [\"Pandemic Severity Assessment Framework\", 0.004537456222258886], [\"Epidemiology of HIV/AIDS\", 0.0016857910270197945], [\"Virus\", 0.0015866268489598066]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "bebe0c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search function to search the inverted index\n",
    "\n",
    "def search(query, index = inverted_index):\n",
    "    \n",
    "    query_tokens = tokenizer(query)\n",
    "    \n",
    "    # Lookup all query tokens in the inverted index\n",
    "    # and build an list of articles that have them~\n",
    "    # The results should be a list of tuples with article titles and TfIdf scores\n",
    "    newlist = []\n",
    "    for token in query_tokens:\n",
    "        newlist.extend(inverted_index[token])\n",
    "        \n",
    "    # create a dictionary with compound TfIdf scores \n",
    "    # to take into account that an article can include multiple keywords\n",
    "    # from your query\n",
    "    #     output = defaultdict(int) \n",
    "    #     for k, v in newlist: \n",
    "    #         output[k] += v \n",
    "    \n",
    "    # sort search results by their TfIdf scores\n",
    "    results = {'query': query, \"relevant_article_titles\": sorted(newlist, key=lambda column: column[1], reverse=True)}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deafbf5",
   "metadata": {},
   "source": [
    "## Test your search engine with a couple of different search queries:\n",
    "\n",
    "- Try to search \"symptoms of swine flu\". Great news, - the article titled Swine influenza should be the first title on the list.\n",
    "\n",
    "\n",
    "- Let us submit another, slightly more ambiguous query. Say, you want to find out which other organizations, besides CDC, are working on pandemic prevention programs. Try to search “pandemic prevention organizations” and check what comes up. Disappointingly, the titles with the highest Tf-Idf ranks are not going to answer your question. The main disadvantage of keyword search is that it does not understand the context and the meaning of your request. It only knows how often the keywords appear in the given document compared to other documents in the database.\n",
    "\n",
    "\n",
    "- Compare your results for these example queries and a few other suggested queries in theexample_queries.json file with our top three results for each search request. The results are provided in the example_query_results.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "066e689b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'world health organization',\n",
       " 'relevant_article_titles': [['Basic reproduction number',\n",
       "   0.0017412627311117635],\n",
       "  ['1929–1930 psittacosis pandemic', 0.0016544015885326235],\n",
       "  ['Event 201', 0.0016429198403741917],\n",
       "  ['Event 201', 0.0016429198403741917],\n",
       "  ['Event 201', 0.0016429198403741917],\n",
       "  ['Science diplomacy and pandemics', 0.0016395346300336333],\n",
       "  ['Science diplomacy and pandemics', 0.0016395346300336333],\n",
       "  ['Science diplomacy and pandemics', 0.0016395346300336333],\n",
       "  ['Disease X', 0.0015825540903668617],\n",
       "  ['Disease X', 0.0015825540903668617],\n",
       "  ['Disease X', 0.0015825540903668617],\n",
       "  ['Cholera', 0.0015811471307073243],\n",
       "  ['Pandemic severity index', 0.0015604349960286247],\n",
       "  ['Epidemiology of HIV/AIDS', 0.001544404664042881],\n",
       "  ['COVID-19 pandemic', 0.0015344736613540093],\n",
       "  ['Spanish flu', 0.0015079630243867441],\n",
       "  ['Crimson Contagion', 0.0014762184447524571],\n",
       "  ['Swine influenza', 0.0013703364096327216],\n",
       "  ['Swine influenza', 0.0013703364096327216],\n",
       "  ['Swine influenza', 0.0013703364096327216],\n",
       "  ['Pandemic prevention', 0.0012177254403986776]]}"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how well this search performs for multi-word queries:\n",
    "results = search(query = \"world health organization\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "a3d7cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in summaries:\n",
    "    if s[\"title\"] == title:\n",
    "        print(s[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "f799ded6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Ebola virus',\n",
       " 'relevant_article_titles': [['Plague of Cyprian', 0.0016430040145475272],\n",
       "  ['Plague of Cyprian', 0.0016430040145475272],\n",
       "  ['Science diplomacy and pandemics', 0.0016395346300336333],\n",
       "  ['Virus', 0.0016226086401492994],\n",
       "  ['Disease X', 0.0015825540903668617],\n",
       "  ['Epidemiology of HIV/AIDS', 0.001544404664042881],\n",
       "  ['Viral load', 0.0015372392135629888],\n",
       "  ['COVID-19 pandemic', 0.0015344736613540093],\n",
       "  ['Spanish flu', 0.0015079630243867441],\n",
       "  ['Crimson Contagion', 0.0014762184447524571],\n",
       "  ['HIV/AIDS in Yunnan', 0.001446525799411811],\n",
       "  ['HIV/AIDS', 0.0014317072468028223],\n",
       "  ['Swine influenza', 0.0013703364096327216]]}"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try another multi-word query\n",
    "search(query = \"Ebola virus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da3560",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in summaries:\n",
    "    if s[\"title\"] == 'Virus':\n",
    "        print(s[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
