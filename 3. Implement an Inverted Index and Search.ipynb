{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a57d87",
   "metadata": {},
   "source": [
    "# M1 Text Processing Using spaCy Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a08d956",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Preprocess the dataset using spaCy library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab25ad",
   "metadata": {},
   "source": [
    "## Load all relevant Python libraries and a spaCy language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "4f6e04d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8bfc222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "a376a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = sp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b7fb3",
   "metadata": {},
   "source": [
    "##  Open the provided JSON file. \n",
    "\n",
    "It contains a list of dictionaries with summaries from Wikipedia articles, where each dictionary has three key-value pairs. The keys title, text and url correspond to:\n",
    "\n",
    "- Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- Link to the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e5e3d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'text', 'url'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/data.json', 'r') as outfile:\n",
    "    summaries = json.load(outfile)\n",
    "print(summaries[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "751513de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pandemic (from Greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. A widespread endemic disease with a stable number of infected people is not a pandemic. Widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\\nThroughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. The most fatal pandemic in recorded history was the Black Death (also known as The Plague), which killed an estimated 75–200 million people in the 14th century. The term was not used yet but was for later pandemics including the 1918 influenza pandemic (Spanish flu). Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e4145c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a37ed",
   "metadata": {},
   "source": [
    "## Create a Python function that takes in a text string, performs all operations described in the previous step, and outputs a list of tokens (lemmas).\n",
    "\n",
    "- Lowercases the text string.\n",
    "\n",
    "\n",
    "- Creates a spaCy document with the text lemmas and their attributes using a spaCy model of your choice.\n",
    "\n",
    "\n",
    "- Removes stop words, punctuation, and other unclassified lemmas.\n",
    "\n",
    "\n",
    "- Returns a list of tokens (lemmas) found in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5d79a31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> a DET det a\n",
      "<class 'spacy.tokens.token.Token'> pandemic ADJ nsubj pandemic\n",
      "<class 'spacy.tokens.token.Token'> ( PUNCT punct (\n",
      "<class 'spacy.tokens.token.Token'> from ADP prep from\n",
      "<class 'spacy.tokens.token.Token'> greek ADJ amod greek\n"
     ]
    }
   ],
   "source": [
    "# Lowercase data. Lowercase the text\n",
    "# Explore the attributes of each token returned SpaCy.\n",
    "text = summaries[0]['text']\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized[:5]:\n",
    "    print(type(token), token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b3f54925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a pandemic (from greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. a widespread endemic disease with a stable number of infected people is not a pandemic. widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n",
       "throughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. the most fatal pandemic in recorded history was the black death (also known as the plague), which killed an estimated 75–200 million people in the 14th century. the term was not used yet but was for later pandemics including the 1918 influenza pandemic (spanish flu). current pandemics include covid-19 (sars-cov-2) and hiv/aids."
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f3cb42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return sp(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "95b61341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a pandemic (from greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. a widespread endemic disease with a stable number of infected people is not a pandemic. widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n",
       "throughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. the most fatal pandemic in recorded history was the black death (also known as the plague), which killed an estimated 75–200 million people in the 14th century. the term was not used yet but was for later pandemics including the 1918 influenza pandemic (spanish flu). current pandemics include covid-19 (sars-cov-2) and hiv/aids."
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower(summaries[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "afe44bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation\n",
    "def remove_redundant_tokens(text_tokenized):\n",
    "    return ' '.join([token.text for token in text_tokenized if not token.is_stop and not token.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4c0070ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize (tokenize) the texts\n",
    "def lemmatize(text):\n",
    "    return [token.lemma_ for token in sp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "01af4a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tokenizer function\n",
    "def tokenizer(document):\n",
    "    \"\"\"\n",
    "    This function accepts a text string and:\n",
    "    1. Lowercases it\n",
    "    2. Removes redundant tokens\n",
    "    3. Performs token lemmatization\n",
    "    \"\"\" \n",
    "    text_tokenized = lower(document)\n",
    "    clean_text =  remove_redundant_tokens(text_tokenized)\n",
    "    token_lemmatized = lemmatize(clean_text)\n",
    "    return token_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f454a",
   "metadata": {},
   "source": [
    "## Use this function to preprocess all text documents in the dataset (text field only), and add the resulting lists to the dictionaries from step 1. \n",
    "\n",
    "You should end up with a list of dictionaries, each of which now has four key-value pairs:\n",
    "\n",
    "- title: Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- text: Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- tokenized_text: Tokenized Wikipedia article text.\n",
    "\n",
    "\n",
    "- url: Link to the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "50ca268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all the documents using the tokenizer function\n",
    "for doc in summaries:\n",
    "    doc['tokenized_text'] = tokenizer(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c9dc1803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pandemic',\n",
       " 'greek',\n",
       " 'πᾶν',\n",
       " 'pan',\n",
       " 'δῆμος',\n",
       " 'demos',\n",
       " 'people',\n",
       " 'epidemic',\n",
       " 'infectious',\n",
       " 'disease']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]['tokenized_text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1f0623c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries[0]['tokenized_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb026b",
   "metadata": {},
   "source": [
    "## Save the new list of dictionaries in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5953e638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='data/summaries.json' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenized texts to file:\n",
    "with open('data/summaries.json', 'w') as outfile:\n",
    "    json.dump(summaries, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943df7",
   "metadata": {},
   "source": [
    "# M2 TF-IDF Search Using Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef753bc",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Implement a basic Tf-Idf search.\n",
    "\n",
    "- In your search for an optimal document retrieval method in the CDC’s huge knowledge base, you decide to try the term frequency search first because of its simplicity. It is a well-developed technique and is a great place to start!\n",
    "\n",
    "\n",
    "- In Milestone 1, you prepared the documents for Tf-Idf-based search. You also computed the Tf-Idf vectors for every document in the CDC’s knowledge base. The standard approach to finding the most relevant documents to your query is to compute similarities between the Tf-Idf vectors of the documents and the query. It works, but you realize that it can be very inefficient for very large document sets since you need to compute the similarities between the query and every one of the documents in your database. What would be a better solution? Let us move on to the last milestone of the project to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f27ac",
   "metadata": {},
   "source": [
    "## Load all relevant Python libraries and a spaCy language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "30c37d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e63add",
   "metadata": {},
   "source": [
    "## Access the tokenized text in your new dataset from the previous milestone. \n",
    "\n",
    "Each document dictionary should now include a new key-value pair with the lemmatized text of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "f11a8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/summaries.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11b3aa",
   "metadata": {},
   "source": [
    "## Create a corpus vocabulary. It should simply be a list of unique tokens in the provided set of documents. \n",
    "\n",
    "Count how many times each unique token appears in the corpus, you will need these counts for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "eb0cc146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n",
      "(3617,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1506"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all tokenized texts into a single list\n",
    "tokenized_texts = [i[\"tokenized_text\"] for i in data]\n",
    "print(np.array(tokenized_texts).shape)\n",
    "\n",
    "# flatten the list of lists (use itertools.chain)\n",
    "flattened_tokenized_texts = list(itertools.chain(*tokenized_texts))\n",
    "print(np.array(flattened_tokenized_texts).shape)\n",
    "\n",
    "# remove duplicates\n",
    "vocab = list(set(flattened_tokenized_texts))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "99c37d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary as a json file\n",
    "with open('data/vocab.json', 'w') as outfile:\n",
    "    json.dump(vocab, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "9b2c3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times each token occurs in a document\n",
    "# you will need it for TfIdf calculations\n",
    "docs_token_counter = []\n",
    "for doc in data:\n",
    "    # For each document, count how many of each token they have\n",
    "    # Counter function from collections is very handy\n",
    "    docs_token_counter.append(Counter(doc['tokenized_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "acf6d3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "079a6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_docs_with_token  = {}\n",
    "for token in vocab:\n",
    "   # For each token in corpus vocabulary, count in how many documents it occurs\n",
    "    doc_count = 0\n",
    "    for document in data:\n",
    "        if token in document['tokenized_text']:\n",
    "            doc_count = doc_count + 1\n",
    "    number_docs_with_token[token] = doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "0ed7fded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 16, 2, 3, 4, 3, 2, 2, 2]"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v for v in number_docs_with_token.values() if v > 1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "aa287e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_docs_with_token['ebola']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d37bcd1",
   "metadata": {},
   "source": [
    "## Calculate Tf-Idf vectors for every article in the dataset and add these vectors to the article dictionaries. \n",
    "\n",
    "You should end up the same list of dictionaries as before, but with a new key-value pair containing Tf-Idf vectors:\n",
    "\n",
    "- title: Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- text: Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- tokenized_text: Tokenized Wikipedia article text.\n",
    "\n",
    "\n",
    "- url: Link to the Wikipedia article.\n",
    "\n",
    "\n",
    "- tf_idfs: Tf_Idf vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6502e12",
   "metadata": {},
   "source": [
    "$tf = \\frac{count(token\\:in\\:document)}{count(all\\tokens\\:in\\:document)}$\n",
    "\n",
    "\n",
    "$idf(token) = \\frac{number\\:of\\:documents}{number\\:of\\:documents\\:containing\\:the\\:token}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "9e6ba0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'disease'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "89abb64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_token_counter[0][token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "c9f5b988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[i][\"tokenized_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "98710a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045454545454545456"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = docs_token_counter[0][token]/len(data[0][\"tokenized_text\"])\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "9ebe5c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_docs_with_token[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "a0d091c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4855078157817008"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.log(len(data)/number_docs_with_token['disease'])\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "f999c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token_counter in enumerate(docs_token_counter):\n",
    "    tfidf_vec = []\n",
    "    for token in vocab:\n",
    "        # compute a term frequency (tf) per document\n",
    "        tf = token_counter[token] / len(data[i][\"tokenized_text\"])\n",
    "\n",
    "        # compute a log of inverse document frequency per document\n",
    "        idf = np.log(len(data)/number_docs_with_token[token])\n",
    "        \n",
    "        # Compute tfidf for the token and append to a list of tf_idfs for this document\n",
    "        tfidf_vec.append(tf*idf)\n",
    "    \n",
    "    # add tf_idf vector to the corresponding data dictionary\n",
    "    data[i]['tfidf_vec'] = tfidf_vec       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "17eadf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an updates summary with computed Tf-Idf vectors\n",
    "with open('data/summaries.json', 'w') as json_file:\n",
    "    json.dump(data, json_file)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "953dd0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"highest pandemic casualties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "420bf43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the workflow for article Tf-Idf calculation\n",
    "# to build a vectorizer function for search queries\n",
    "\n",
    "def vectorize(query, vocab = vocab): \n",
    "    query_vec = []\n",
    "    # Tokenize query\n",
    "    tokenized_query = tokenizer(query)\n",
    "    query_length = len(tokenized_query)\n",
    "    # Count unique tokens in query\n",
    "    for token in vocab:\n",
    "        # Build a TfIdf vector of the same shape as the document TfIdfs\n",
    "        tf = Counter(tokenized_query)[token]/query_length\n",
    "        idf = np.log(len(data)/number_docs_with_token[token])\n",
    "        query_vec.append(tf*idf)\n",
    "        \n",
    "    return query_vec        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fea1a6",
   "metadata": {},
   "source": [
    "## Now we can try to search our list of dictionaries using this Tf-Idf field using existing tools for similarity. \n",
    "\n",
    "We suggest you use scikit-learn library and its cosine_similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "50fb3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = np.array(vectorize(query)).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "800318bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_2 = np.array(data[0]['tfidf_vec']).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "17bb3acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0160686743357183"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec_1, vec_2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "86818ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a search function\n",
    "def search_tfidf(query, docs):\n",
    "    rankings = []\n",
    "    # vectorize query\n",
    "    try:\n",
    "        vectorized_query = vectorize(query)\n",
    "    except:\n",
    "        print(query)\n",
    "        return rankings\n",
    "    # Build a list of results using sklearn cosine_similarity function\n",
    "    for doc in docs:\n",
    "        # compute cosine similarity rank\n",
    "        rank = cosine_similarity(np.array(vectorized_query).reshape(1, -1), np.array(doc['tfidf_vec']).reshape(1, -1))[0][0]\n",
    "        if rank > 0:\n",
    "            # add this document to results along with its similarity rank\n",
    "            rankings.append({'title': doc['title'], 'rank': rank})\n",
    "     \n",
    "    # The results should be a list of dictionaries with at least the 'title' and 'rank' fields\n",
    "    return sorted(rankings, key=lambda item: item.get(\"rank\"), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "f5f17cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Plague of Cyprian', 'rank': 0.11778345241757451},\n",
       " {'title': 'Science diplomacy and pandemics', 'rank': 0.07118947137494436}]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test how well this fuction works\n",
    "search_tfidf(\"ebola\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "75464248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Plague of Cyprian was a pandemic that afflicted the Roman Empire about from AD 249 to 262. The plague is thought to have caused widespread manpower shortages for food production and the Roman army, severely weakening the empire during the Crisis of the Third Century. Its modern name commemorates St. Cyprian, bishop of Carthage, an early Christian writer who witnessed and described the plague. The agent of the plague is highly speculative because of sparse sourcing, but suspects have included smallpox, pandemic influenza and viral hemorrhagic fever (filoviruses) like the Ebola virus.\n"
     ]
    }
   ],
   "source": [
    "for s in data:\n",
    "    if s[\"title\"] == 'Plague of Cyprian':\n",
    "        print(s[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9afdff7",
   "metadata": {},
   "source": [
    "# M3 Implement an Inverted Index and Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23302dc",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Implement an inverted index and search.\n",
    "\n",
    "- After testing out a simple Tf-Idf search, you realize that you can successfully search your repository of documents. However, the CDC’s library is enormous, and looping over every document to compute cosine similarities for every one of them does not seem like the best way to search. There must be a better way to do it!\n",
    "\n",
    "\n",
    "- Inverted index is the most commonly used data structure in document retrieval systems because it enables very fast full-text search. Instead of looking up query tokens in every document in the database, we can quickly retrieve the documents that are already known to contain the tokens by their keys. Our inclusion of Tf-Idf helps to take into account the number of times the token occurred in the document or its relevance.\n",
    "\n",
    "\n",
    "- The downside of using an inverted index is the increased processing cost. We have to tokenize every document and compute all the Tf-Idf values to be able to search them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e1dd8",
   "metadata": {},
   "source": [
    "## Create a new Jupyter Notebook and load dependencies required to complete this milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "acae7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d169ce9",
   "metadata": {},
   "source": [
    "## Load the two JSON files you created and saved in Milestone 2:\n",
    "\n",
    "- The vocabulary file, containing all tokens in our corpus.\n",
    "\n",
    "\n",
    "- The file with the documents. Each document dictionary now should contain the following fields: title, text, URL, tokenized_text, tf_idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "bc991bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/summaries.json', 'r') as f:\n",
    "    summaries = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "23c57adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d01dee",
   "metadata": {},
   "source": [
    "## Build an inverted index. \n",
    "\n",
    "- Use the previously calculated Tf_Idf values to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "0cd18b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['COVID-19 pandemic', 0.05682726519804911]]"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index = {}\n",
    "\n",
    "# Create a lookup dictionary for each word in vocabulary\n",
    "for i, word in enumerate(vocab):\n",
    "    inverted_index[word] = []  \n",
    "    # for each word in corpus vocabulary list all articles\n",
    "    # it occurs in and this word's TfIdf score for this article\n",
    "    for doc in summaries:\n",
    "        if word in doc['tokenized_text']:\n",
    "            inverted_index[word].append([doc['title'], doc['tfidf_vec'][i]])\n",
    "# Now you have a lookup table of all articles that have a particular keyword\n",
    "# lets request a list of articles with the word \"coronavirus\" in them\n",
    "inverted_index[\"coronavirus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "b10fe052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The disease was first identified in December 2019 in Wuhan, China. The outbreak was declared a Public Health Emergency of International Concern in January 2020, and a pandemic in March 2020. As of 17 October 2020, more than 39.5 million cases have been confirmed, with more than 1.1 million deaths attributed to COVID-19.\n",
      "\n",
      "Common symptoms include fever, cough, fatigue, breathing difficulties, and loss of smell. Complications may include pneumonia and acute respiratory distress syndrome. The incubation period is typically around five days but may range from one to 14 days. There are several vaccine candidates in development, although none have proven their safety and efficacy. There is no known specific antiviral medication, so primary treatment is currently symptomatic.\n",
      "Recommended preventive measures include hand washing, covering mouth or wearing face mask when sneezing or coughing, social distancing, disinfecting surfaces, ventilation and air-filtering, and monitoring and self-isolation if exposed or symptomatic. Travel restrictions, lockdowns, workplace hazard controls, and facility closures have been implemented. Many places have also worked to increase testing capacity and trace contacts of the infected. These have caused social and economic disruption, including the largest global recession since the Great Depression. Extreme poverty and global famines are affecting hundreds of millions, inflamed by supply shortages. Many events, the environment and education systems have also been affected. Misinformation about the virus has circulated globally. There have been many incidents of xenophobia and racism against Chinese people and against those perceived as being Chinese or as being from areas with high infection rates.\n"
     ]
    }
   ],
   "source": [
    "# Check if \"coronavirus\" is indeed in the article \n",
    "for s in summaries:\n",
    "    if s[\"title\"] == 'COVID-19 pandemic':\n",
    "        print(s[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f7e04",
   "metadata": {},
   "source": [
    "## Now that we have the index, we need a search function. \n",
    "\n",
    "Build a search function that accepts query texts, searches the inverted index, and returns sorted search results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fe7a7",
   "metadata": {},
   "source": [
    "{\"query\": \"black death\", \"relevant_article_titles\": [[\"Pandemic\", 0.047336756359133265], [\"Cholera\", 0.014518164813892178], [\"Antonine Plague\", 0.013233865618817103], [\"Epidemiology of HIV/AIDS\", 0.011947239794765438], [\"Bills of mortality\", 0.008868054280650635], [\"Spanish flu\", 0.008602012652231115], [\"1929\\u20131930 psittacosis pandemic\", 0.008231591054766618], [\"Pandemic Severity Assessment Framework\", 0.008039264160963658], [\"HIV/AIDS\", 0.006826994168437393], [\"COVID-19 pandemic\", 0.005060007442488892], [\"Swine influenza\", 0.004675006876212563]]}, {\"query\": \"zoonotic diseases\", \"relevant_article_titles\": [[\"Swine influenza\", 0.035414092804581326], [\"Disease X\", 0.029604135108640295], [\"Pandemic\", 0.022322198426744867], [\"Pandemic prevention\", 0.013871651879477165], [\"HIV/AIDS\", 0.013486328216158356], [\"Targeted immunization strategies\", 0.0105545177343848], [\"Science diplomacy and pandemics\", 0.01032995352727023], [\"HIV/AIDS in Yunnan\", 0.008593058686401785], [\"Cholera\", 0.008194224738931659], [\"Antonine Plague\", 0.007469351012026167], [\"Superspreader\", 0.0066507919970096], [\"COVID-19 pandemic\", 0.005711856656255304], [\"Basic reproduction number\", 0.0056020132590196255], [\"1929\\u20131930 psittacosis pandemic\", 0.004646007806523453], [\"Pandemic Severity Assessment Framework\", 0.004537456222258886], [\"Epidemiology of HIV/AIDS\", 0.0016857910270197945], [\"Virus\", 0.0015866268489598066]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "3ad5d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search function to search the inverted index\n",
    "\n",
    "def search(query, index = inverted_index):\n",
    "    \n",
    "    query_tokens = tokenizer(query)\n",
    "    \n",
    "    # Lookup all query tokens in the inverted index\n",
    "    # and build an list of articles that have them~\n",
    "    # The results should be a list of tuples with article titles and TfIdf scores\n",
    "    newlist = []\n",
    "    for token in query_tokens:\n",
    "        newlist.extend(inverted_index[token])\n",
    "        \n",
    "    # create a dictionary with compound TfIdf scores \n",
    "    # to take into account that an article can include multiple keywords\n",
    "    # from your query\n",
    "    #     output = defaultdict(int) \n",
    "    #     for k, v in newlist: \n",
    "    #         output[k] += v \n",
    "    \n",
    "    # sort search results by their TfIdf scores\n",
    "    sorted_tuples = [(x, y) for x,y in sorted(newlist, key=lambda column: column[1], reverse=True)]\n",
    "    results = {'query': query, \"relevant_article_titles\": sorted_tuples}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55e6aa",
   "metadata": {},
   "source": [
    "## Test your search engine with a couple of different search queries:\n",
    "\n",
    "- Try to search \"symptoms of swine flu\". Great news, - the article titled Swine influenza should be the first title on the list.\n",
    "\n",
    "\n",
    "- Let us submit another, slightly more ambiguous query. Say, you want to find out which other organizations, besides CDC, are working on pandemic prevention programs. Try to search “pandemic prevention organizations” and check what comes up. Disappointingly, the titles with the highest Tf-Idf ranks are not going to answer your question. The main disadvantage of keyword search is that it does not understand the context and the meaning of your request. It only knows how often the keywords appear in the given document compared to other documents in the database.\n",
    "\n",
    "\n",
    "- Compare your results for these example queries and a few other suggested queries in theexample_queries.json file with our top three results for each search request. The results are provided in the example_query_results.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "d4570ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how well this search performs for multi-word queries:\n",
    "results = search(query = \"world health organization\")\n",
    "title, score = results['relevant_article_titles'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "d8d7992d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'world health organization',\n",
       " 'relevant_article_titles': [('Event 201', 0.11049890590702932),\n",
       "  ('Event 201', 0.04582303339979459),\n",
       "  ('Pandemic prevention', 0.040731585244261855),\n",
       "  ('Event 201', 0.020502912327596386),\n",
       "  ('Crimson Contagion', 0.019043338555758793),\n",
       "  ('Pandemic severity index', 0.019010564457123326),\n",
       "  ('Disease X', 0.017456393676112226),\n",
       "  ('Disease X', 0.015621266535311531),\n",
       "  ('Science diplomacy and pandemics', 0.01559933051907901),\n",
       "  ('Disease X', 0.01403160709930531),\n",
       "  ('Science diplomacy and pandemics', 0.013959429669852858),\n",
       "  ('Spanish flu', 0.013056580984737996),\n",
       "  ('Science diplomacy and pandemics', 0.012538882939804746),\n",
       "  ('Swine influenza', 0.007758397189383211),\n",
       "  ('Swine influenza', 0.0069427851268051245),\n",
       "  ('COVID-19 pandemic', 0.00685264532756771),\n",
       "  ('Swine influenza', 0.006236269821913471),\n",
       "  ('1929–1930 psittacosis pandemic', 0.005559693378970029),\n",
       "  ('Cholera', 0.005536651430236998),\n",
       "  ('Epidemiology of HIV/AIDS', 0.004509231577203328),\n",
       "  ('Basic reproduction number', 0.0036378240627828583)]}"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "a060b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Johns Hopkins Center for Health Security (abbreviated CHS; previously the UPMC Center for Health Security, the Center for Biosecurity of UPMC, and the Johns Hopkins Center for Civilian Biodefense Strategies) is an independent, nonprofit organization of the Johns Hopkins Bloomberg School of Public Health, and part of the Environmental Health and Engineering department. It is concerned with the areas of health consequences from epidemics and disasters as well as averting biological weapons development, and implications of biosecurity for the bioeconomy. It is a think tank that does policy research and gives policy recommendations to the United States government as well as the World Health Organization and the UN Biological Weapons Convention.\n"
     ]
    }
   ],
   "source": [
    "for s in summaries:\n",
    "    if s[\"title\"] == title:\n",
    "        print(s[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "02f33278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try another multi-word query\n",
    "results = search(query = \"Ebola virus\")\n",
    "title, score = results['relevant_article_titles'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "fca68ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Ebola virus',\n",
       " 'relevant_article_titles': [('Virus', 0.05272201302980361),\n",
       "  ('Plague of Cyprian', 0.047499062175213644),\n",
       "  ('Viral load', 0.033601611922777795),\n",
       "  ('Crimson Contagion', 0.03351433500869266),\n",
       "  ('Disease X', 0.03072147375796827),\n",
       "  ('Science diplomacy and pandemics', 0.027286695292144007),\n",
       "  ('Swine influenza', 0.022756647228124642),\n",
       "  ('HIV/AIDS in Yunnan', 0.022636875400608197),\n",
       "  ('Plague of Cyprian', 0.01592965305968725),\n",
       "  ('HIV/AIDS', 0.013600020003527455),\n",
       "  ('Spanish flu', 0.012838824854076293),\n",
       "  ('Epidemiology of HIV/AIDS', 0.005912036187100423),\n",
       "  ('COVID-19 pandemic', 0.0050011701466459975)]}"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "7bb4a9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A virus is a submicroscopic infectious agent that replicates only inside the living cells of an organism. Viruses infect all types of life forms, from animals and plants to microorganisms, including bacteria and archaea.\n",
      "Since Dmitri Ivanovsky's 1892 article describing a non-bacterial pathogen infecting tobacco plants and the discovery of the tobacco mosaic virus by Martinus Beijerinck in 1898, more than 6,000  virus species have been described in detail of the millions of types of viruses in the environment. Viruses are found in almost every ecosystem on Earth and are the most numerous type of biological entity. The study of viruses is known as virology, a subspeciality of microbiology.\n",
      "When infected, a host cell is forced to rapidly produce thousands of identical copies of the original virus. When not inside an infected cell or in the process of infecting a cell, viruses exist in the form of independent particles, or virions, consisting of: (i) the genetic material, i.e., long molecules of DNA or RNA that encode the structure of the proteins by which the virus acts; (ii) a protein coat, the capsid, which surrounds and protects the genetic material; and in some cases (iii) an outside envelope of lipids. The shapes of these virus particles range from simple helical and icosahedral forms to more complex structures. Most virus species have virions too small to be seen with an optical microscope, as they are one-hundredth the size of most bacteria.\n",
      "The origins of viruses in the evolutionary history of life are unclear: some may have evolved from plasmids—pieces of DNA that can move between cells—while others may have evolved from bacteria. In evolution, viruses are an important means of horizontal gene transfer, which increases genetic diversity in a way analogous to sexual reproduction. Viruses are considered by some biologists to be a life form, because they carry genetic material, reproduce, and evolve through natural selection, although they lack the key characteristics, such as cell structure, that are generally considered necessary criteria for life. Because they possess some but not all such qualities, viruses have been described as \"organisms at the edge of life\", and as self-replicators.Viruses spread in many ways. One transmission pathway is through disease-bearing organisms known as vectors: for example, viruses are often transmitted from plant to plant by insects that feed on plant sap, such as aphids; and viruses in animals can be carried by blood-sucking insects. Influenza viruses are spread by coughing and sneezing. Norovirus and rotavirus, common causes of viral gastroenteritis, are transmitted by the faecal–oral route, passed by hand-to-mouth contact or in food or water. The infectious dose of norovirus required to produce infection in humans is less than 100 particles. HIV is one of several viruses transmitted through sexual contact and by exposure to infected blood. The variety of host cells that a virus can infect is called its \"host range\". This can be narrow, meaning a virus is capable of infecting few species, or broad, meaning it is capable of infecting many.Viral infections in animals provoke an immune response that usually eliminates the infecting virus. Immune responses can also be produced by vaccines, which confer an artificially acquired immunity to the specific viral infection. Some viruses, including those that cause AIDS, HPV infection, and viral hepatitis, evade these immune responses and result in chronic infections. Several antiviral drugs have been developed.\n"
     ]
    }
   ],
   "source": [
    "for s in summaries:\n",
    "    if s[\"title\"] == 'Virus':\n",
    "        print(s[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c509d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
