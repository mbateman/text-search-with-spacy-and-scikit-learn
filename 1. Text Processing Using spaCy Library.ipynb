{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "950e9827",
   "metadata": {},
   "source": [
    "# M1 Text Processing Using spaCy Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cca9db",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Preprocess the dataset using spaCy library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc2282",
   "metadata": {},
   "source": [
    "## Load all relevant Python libraries and a spaCy language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2cfd80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597ded45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e8e5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = sp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9158c4",
   "metadata": {},
   "source": [
    "##  Open the provided JSON file. \n",
    "\n",
    "It contains a list of dictionaries with summaries from Wikipedia articles, where each dictionary has three key-value pairs. The keys title, text and url correspond to:\n",
    "\n",
    "- Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- Link to the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ddfde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'text', 'url'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/data.json', 'r') as outfile:\n",
    "    summaries = json.load(outfile)\n",
    "print(summaries[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acf7f136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pandemic (from Greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. A widespread endemic disease with a stable number of infected people is not a pandemic. Widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\\nThroughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. The most fatal pandemic in recorded history was the Black Death (also known as The Plague), which killed an estimated 75–200 million people in the 14th century. The term was not used yet but was for later pandemics including the 1918 influenza pandemic (Spanish flu). Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4973e26",
   "metadata": {},
   "source": [
    "## Create a Python function that takes in a text string, performs all operations described in the previous step, and outputs a list of tokens (lemmas).\n",
    "\n",
    "- Lowercases the text string.\n",
    "\n",
    "\n",
    "- Creates a spaCy document with the text lemmas and their attributes using a spaCy model of your choice.\n",
    "\n",
    "\n",
    "- Removes stop words, punctuation, and other unclassified lemmas.\n",
    "\n",
    "\n",
    "- Returns a list of tokens (lemmas) found in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65b63579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> a DET det a\n",
      "<class 'spacy.tokens.token.Token'> pandemic ADJ nsubj pandemic\n",
      "<class 'spacy.tokens.token.Token'> ( PUNCT punct (\n",
      "<class 'spacy.tokens.token.Token'> from ADP prep from\n",
      "<class 'spacy.tokens.token.Token'> greek ADJ amod greek\n"
     ]
    }
   ],
   "source": [
    "# Lowercase data. Lowercase the text\n",
    "# Explore the attributes of each token returned SpaCy.\n",
    "text = summaries[0]['text']\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized[:5]:\n",
    "    print(type(token), token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "497776a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a pandemic (from greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. a widespread endemic disease with a stable number of infected people is not a pandemic. widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n",
       "throughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. the most fatal pandemic in recorded history was the black death (also known as the plague), which killed an estimated 75–200 million people in the 14th century. the term was not used yet but was for later pandemics including the 1918 influenza pandemic (spanish flu). current pandemics include covid-19 (sars-cov-2) and hiv/aids."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2282fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jarvel ROOT\n",
      "in prep\n",
      "the det\n",
      "gaabe pobj\n"
     ]
    }
   ],
   "source": [
    "# Find tokens which do not have a description (token.dep_)\n",
    "# They belong to tokens that need to be removed\n",
    "text = 'jarvel in the gaabe'\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized:\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3c6eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return sp(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b23a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation\n",
    "def remove_redundant_tokens(text_tokenized):\n",
    "    return ' '.join([token.text for token in text_tokenized if not token.is_stop and not token.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d067cce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pandemic greek πᾶν pan δῆμος demos people epidemic infectious disease spread large region instance multiple continents worldwide affecting substantial number people widespread endemic disease stable number infected people pandemic widespread endemic diseases stable number infected people recurrences seasonal influenza generally excluded occur simultaneously large regions globe spread worldwide \\n human history number pandemics diseases smallpox tuberculosis fatal pandemic recorded history black death known plague killed estimated 75–200 million people 14th century term later pandemics including 1918 influenza pandemic spanish flu current pandemics include covid-19 sars cov-2 hiv aids'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_redundant_tokens(lower(summaries[0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75689351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize (tokenize) the texts\n",
    "def lemmatize(text):\n",
    "    return ' '.join([token.lemma_ for token in text_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a2451eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pandemic greek πᾶν pan δῆμος demos people epidemic infectious disease spread large region instance multiple continent worldwide affect substantial number people widespread endemic disease stable number infect people pandemic widespread endemic disease stable number infect people recurrence seasonal influenza generally exclude occur simultaneously large region globe spread worldwide \\n  human history number pandemic disease smallpox tuberculosis fatal pandemic record history black death know plague kill estimate 75–200 million people 14th century term later pandemic include 1918 influenza pandemic spanish flu current pandemic include covid-19 sar cov-2 hiv aid'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(remove_redundant_tokens(lower(summaries[0]['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7ccadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tokenizer function\n",
    "def tokenizer(document):\n",
    "    \"\"\"\n",
    "    This function accepts a text string and:\n",
    "    1. Lowercases it\n",
    "    2. Removes redundant tokens\n",
    "    3. Performs token lemmatization\n",
    "    \"\"\" \n",
    "    text_tokenized = lower(document)\n",
    "    clean_text =  remove_redundant_tokens(text_tokenized)\n",
    "    token_lemmatized = lemmatize(clean_text)\n",
    "    return token_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2e146",
   "metadata": {},
   "source": [
    "## Use this function to preprocess all text documents in the dataset (text field only), and add the resulting lists to the dictionaries from step 1. \n",
    "\n",
    "You should end up with a list of dictionaries, each of which now has four key-value pairs:\n",
    "\n",
    "- title: Title of the Wikipedia article the text is taken from.\n",
    "\n",
    "\n",
    "- text: Wikipedia article text. (In this dataset we included only the summary.)\n",
    "\n",
    "\n",
    "- tokenized_text: Tokenized Wikipedia article text.\n",
    "\n",
    "\n",
    "- url: Link to the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d354f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all the documents using the tokenizer function\n",
    "for doc in summaries:\n",
    "    doc['tokenized_text'] = tokenizer(doc['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2fcba",
   "metadata": {},
   "source": [
    "## Save the new list of dictionaries in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1da40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized text\n",
    "with open('data/tokenized_text.json', 'w') as f:\n",
    "    summaries = json.dump(summaries, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and save a corpus vocabulary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
